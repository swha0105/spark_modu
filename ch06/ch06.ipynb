{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rolled-thanks",
   "metadata": {},
   "source": [
    "참조 https://github.com/pko89403/Spark-Test/blob/master/AdvancedAnalyticswithSpark/ch6/LSA.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "transparent-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "\n",
    "jar_path = '/usr/spark/jars/spark-xml_2.12-0.11.0.jar'\n",
    "spark = SparkSession.builder.appName('Chapter06')\\\n",
    "    .master('local[4]')\\\n",
    "    .config(\"spark.executor.memory\", \"2g\")\\\n",
    "    .config(\"spark.jars\", jar_path)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.format(\"xml\") \\\n",
    "                .option(\"rowTag\", \"page\") \\\n",
    "                .load(\"wiki_ml.xml\")\n",
    "#                 .load(''/home/jovyan/data/wiki_ml.xml')\\\n",
    "#                 .load(os.getcwd() + 'wiki_ml.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "orange-concentrate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- ns: long (nullable = true)\n",
      " |-- redirect: struct (nullable = true)\n",
      " |    |-- _VALUE: string (nullable = true)\n",
      " |    |-- _title: string (nullable = true)\n",
      " |-- revision: struct (nullable = true)\n",
      " |    |-- comment: string (nullable = true)\n",
      " |    |-- contributor: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- ip: string (nullable = true)\n",
      " |    |    |-- username: string (nullable = true)\n",
      " |    |-- format: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- minor: string (nullable = true)\n",
      " |    |-- model: string (nullable = true)\n",
      " |    |-- parentid: long (nullable = true)\n",
      " |    |-- sha1: string (nullable = true)\n",
      " |    |-- text: struct (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _bytes: long (nullable = true)\n",
      " |    |    |-- _space: string (nullable = true)\n",
      " |    |-- timestamp: timestamp (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sixth-waste",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html data 형태: title, revision의 text안의 _value가 본문, _space, _byte는 metadata\n",
    "parsedDF = df.select(\"title\", \"revision.text._VALUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gothic-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedDF = parsedDF.withColumn(\"raw_text\",F.col(\"_VALUE\")).drop(\"_VALUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smart-hardware",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|            raw_text|\n",
      "+--------------------+--------------------+\n",
      "|     Bongard problem|[[File:Bongard_pr...|\n",
      "|    Generative model|{{About|generativ...|\n",
      "|      Inductive bias|The '''inductive ...|\n",
      "|Category:Bayesian...|{{Cat main|Bayesi...|\n",
      "|Category:Classifi...|{{Commons categor...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsedDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "expanded-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적으로 column 끼리 같은 row단위로 처리\n",
    "# column name인 raw_text string으로 받기 때문에 raw_text가 \"[^a-zA-Z0-9\\\\s]\" 변함\n",
    "# 나머지 아래 값들은 raw_text안의 값들로 지정. 따라서 정규식이 함수처럼 역할\n",
    "# dataframe내에서 정규식 쓰러면 regexp_replace \n",
    "df_clean  = parsedDF.select('title',F.lower(F.regexp_replace('raw_text', \"[^a-zA-Z0-9\\\\s ]\", \" \")).alias('text'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "close-ultimate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|           title|                text|\n",
      "+----------------+--------------------+\n",
      "| Bongard problem|  file bongard pr...|\n",
      "|Generative model|  about generativ...|\n",
      "+----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interstate-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as ml\n",
    "\n",
    "\n",
    "# Tokenizer는 python의 string split 역할.\n",
    "\n",
    "tokenizer = ml.Tokenizer(inputCol='text', outputCol='words_token')\n",
    "df_words_token = tokenizer.transform(df_clean).select(\"title\",\"words_token\")\n",
    "\n",
    "# stop word는 자연어 처리에서 나온 개념. 의미없는 단어 삭제. (I, my.. )\n",
    "remover = ml.StopWordsRemover(inputCol='words_token', outputCol='words_clean')\n",
    "df_wo_stopWords = remover.transform(df_words_token).select('title','words_clean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "atomic-theater",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|           title|         words_clean|\n",
      "+----------------+--------------------+\n",
      "| Bongard problem|[, , file, bongar...|\n",
      "|Generative model|[, , generative, ...|\n",
      "+----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_wo_stopWords_space = df_wo_stopWords.select()\n",
    "\n",
    "df_wo_stopWords.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-search",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "backed-recall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|           title|        refined_text|\n",
      "+----------------+--------------------+\n",
      "| Bongard problem|[file, bongard, p...|\n",
      "|Generative model|[generative, mode...|\n",
      "+----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# python udf은 row-a-time으로 느리다. \n",
    "# pandas udf: The input and output series must have the same size.\n",
    "from pyspark.sql.functions import pandas_udf,PandasUDFType\n",
    "from pyspark.sql.types import ArrayType,StringType\n",
    "import pyspark.sql.column as c\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#pandas_udf 안에는 return data type, https://spark.apache.org/docs/latest/sql-ref-datatypes.html\n",
    "\n",
    "# def remove_empty_word(input_array: pd.Series) -> pd.Series:\n",
    "# @pandas_udf(ArrayType(StringType()))\n",
    "# def remove_empty_word(input_array: pd.Series) -> pd.Series:\n",
    "#     return pd.Series(input_array)\n",
    "\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def remove_empty_word(input_array: pd.Series) -> pd.Series:\n",
    "    import os\n",
    "    os.environ['ARROW_PRE_0_15_IPC_FORMAT']='1'\n",
    "    return_list = []\n",
    "    \n",
    "    for word_list in input_array:\n",
    "        tmp_list = []\n",
    "        for word in word_list:\n",
    "            if word != '':\n",
    "                tmp_list.append(word)\n",
    "        return_list.append(tmp_list)\n",
    "    return pd.Series(return_list)\n",
    "\n",
    "\n",
    "removed = remove_empty_word(df_wo_stopWords.words_clean)\n",
    "df_words_wo_empty = df_wo_stopWords.withColumn('refined_text',removed).select('title','refined_text')  \n",
    "df_words_wo_empty.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-trading",
   "metadata": {},
   "source": [
    "# pyarrow의 버전문제(?)로 한 3시간을 고생했다. \n",
    "# pyarrow.. 설명\n",
    "# https://stackoverflow.com/questions/58458415/pandas-scalar-udf-failing-illegalargumentexception\n",
    "# https://issues.apache.org/jira/browse/SPARK-29367"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "selective-aquarium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|           title|     SnowballStemmed|\n",
      "+----------------+--------------------+\n",
      "| Bongard problem|[file, bongard, p...|\n",
      "|Generative model|[generat, model, ...|\n",
      "+----------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def word_stem(input_array: pd.Series) -> pd.Series:\n",
    "    import os\n",
    "\n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    os.environ['ARROW_PRE_0_15_IPC_FORMAT']='1'\n",
    "    return_list = []\n",
    "    for array in input_array:\n",
    "        tmp_list = []\n",
    "        for word in array:\n",
    "            tmp_list.append(stemmer.stem(word))\n",
    "\n",
    "        return_list.append(tmp_list)\n",
    "    \n",
    "    return pd.Series(return_list)\n",
    "\n",
    "stemmed = word_stem(df_words_wo_empty.refined_text)\n",
    "\n",
    "# stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens], ArrayType(StringType()))\n",
    "stemmed_words = df_words_wo_empty.withColumn(\"SnowballStemmed\", stemmed).drop('refined_text')\n",
    "stemmed_words.show(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "revised-privilege",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[title: string, SnowballStemmed: array<string>, termFreqs: vector]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "# hashingTF = HashingTF(inputCol=\"SnowballStemmed\", outputCol=\"TF\", numFeatures=20)\n",
    "# featurizedData = hashingTF.transform(stemmed_words)\n",
    "\n",
    "countVectorizer = CountVectorizer(inputCol=\"SnowballStemmed\",\n",
    "                                      outputCol=\"termFreqs\",\n",
    "                                      vocabSize=2000)\n",
    "\n",
    "vocabModel = countVectorizer.fit(stemmed_words)\n",
    "docTermFreqs = vocabModel.transform(stemmed_words)\n",
    "docTermFreqs.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "controlled-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idf = IDF(inputCol=\"termFreqs\", \n",
    "          outputCol=\"tfidfVec\")\n",
    "idfModel = idf.fit(docTermFreqs)\n",
    "docTermMatrix = idfModel.transform(docTermFreqs).select(\"title\", \"tfidfVec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "victorian-scroll",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|            tfidfVec|\n",
      "+--------------------+--------------------+\n",
      "|     Bongard problem|(2000,[1,2,3,4,5,...|\n",
      "|    Generative model|(2000,[0,1,2,3,4,...|\n",
      "|      Inductive bias|(2000,[1,2,3,4,5,...|\n",
      "|Category:Bayesian...|(2000,[2,10,14,27...|\n",
      "|Category:Classifi...|(2000,[2,8,10,13,...|\n",
      "|Category:Evolutio...|(2000,[2,10,12,13...|\n",
      "|Semi-supervised l...|(2000,[0,1,2,3,4,...|\n",
      "|  Learning automaton|(2000,[1,2,3,4,5,...|\n",
      "|Category:Machine ...|(2000,[2,8,10,45,...|\n",
      "|Conditional rando...|(2000,[0,1,2,3,4,...|\n",
      "|Cross-entropy method|(2000,[0,1,2,3,4,...|\n",
      "|       Concept drift|(2000,[2,3,7,8,10...|\n",
      "|    Concept learning|(2000,[0,1,2,3,4,...|\n",
      "|      Robot learning|(2000,[1,2,5,6,7,...|\n",
      "|Version space lea...|(2000,[0,1,2,3,4,...|\n",
      "|Evolvability (com...|(2000,[0,2,3,5,7,...|\n",
      "|Prior knowledge f...|(2000,[0,1,2,7,8,...|\n",
      "|  Granular computing|(2000,[0,2,3,4,5,...|\n",
      "|Probability matching|(2000,[0,2,3,4,5,...|\n",
      "|Structural risk m...|(2000,[0,1,2,3,5,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docTermMatrix.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "demographic-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "docTermFreqswithID = docTermFreqs.withColumn('id', monotonically_increasing_id()).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "documented-curve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---+\n",
      "|               title|     SnowballStemmed|           termFreqs| id|\n",
      "+--------------------+--------------------+--------------------+---+\n",
      "|     Bongard problem|[file, bongard, p...|(2000,[1,2,3,4,5,...|  0|\n",
      "|    Generative model|[generat, model, ...|(2000,[0,1,2,3,4,...|  1|\n",
      "|      Inductive bias|[induct, bias, al...|(2000,[1,2,3,4,5,...|  2|\n",
      "|Category:Bayesian...|[cat, main, bayes...|(2000,[2,10,14,27...|  3|\n",
      "|Category:Classifi...|[common, categori...|(2000,[2,8,10,13,...|  4|\n",
      "|Category:Evolutio...|[common, cat, evo...|(2000,[2,10,12,13...|  5|\n",
      "|Semi-supervised l...|[machin, learn, b...|(2000,[0,1,2,3,4,...|  6|\n",
      "|  Learning automaton|[automata, learn,...|(2000,[1,2,3,4,5,...|  7|\n",
      "|Category:Machine ...|[research, studi,...|(2000,[2,8,10,45,...|  8|\n",
      "|Conditional rando...|[multipl, issu, c...|(2000,[0,1,2,3,4,...|  9|\n",
      "|Cross-entropy method|[cross, entropi, ...|(2000,[0,1,2,3,4,...| 10|\n",
      "|       Concept drift|[predict, analyt,...|(2000,[2,3,7,8,10...| 11|\n",
      "|    Concept learning|[multipl, issu, u...|(2000,[0,1,2,3,4,...| 12|\n",
      "|      Robot learning|[robot, learn, re...|(2000,[1,2,5,6,7,...| 13|\n",
      "|Version space lea...|[imag, version, s...|(2000,[0,1,2,3,4,...| 14|\n",
      "|Evolvability (com...|[primari, sourc, ...|(2000,[0,2,3,5,7,...| 15|\n",
      "|Prior knowledge f...|[pattern, recogni...|(2000,[0,1,2,7,8,...| 16|\n",
      "|  Granular computing|[granular, comput...|(2000,[0,2,3,4,5,...| 17|\n",
      "|Probability matching|[footnot, date, f...|(2000,[0,2,3,4,5,...| 18|\n",
      "|Structural risk m...|[otherus, minimis...|(2000,[0,1,2,3,5,...| 19|\n",
      "+--------------------+--------------------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docTermFreqswithID.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "immediate-dependence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- SnowballStemmed: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- termFreqs: vector (nullable = true)\n",
      " |-- id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docTermFreqswithID.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "difficult-washington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|            tfidfVec|\n",
      "+--------------------+--------------------+\n",
      "|     Bongard problem|(2000,[1,2,3,4,5,...|\n",
      "|    Generative model|(2000,[0,1,2,3,4,...|\n",
      "|      Inductive bias|(2000,[1,2,3,4,5,...|\n",
      "|Category:Bayesian...|(2000,[2,10,14,27...|\n",
      "|Category:Classifi...|(2000,[2,8,10,13,...|\n",
      "|Category:Evolutio...|(2000,[2,10,12,13...|\n",
      "|Semi-supervised l...|(2000,[0,1,2,3,4,...|\n",
      "|  Learning automaton|(2000,[1,2,3,4,5,...|\n",
      "|Category:Machine ...|(2000,[2,8,10,45,...|\n",
      "|Conditional rando...|(2000,[0,1,2,3,4,...|\n",
      "|Cross-entropy method|(2000,[0,1,2,3,4,...|\n",
      "|       Concept drift|(2000,[2,3,7,8,10...|\n",
      "|    Concept learning|(2000,[0,1,2,3,4,...|\n",
      "|      Robot learning|(2000,[1,2,5,6,7,...|\n",
      "|Version space lea...|(2000,[0,1,2,3,4,...|\n",
      "|Evolvability (com...|(2000,[0,2,3,5,7,...|\n",
      "|Prior knowledge f...|(2000,[0,1,2,7,8,...|\n",
      "|  Granular computing|(2000,[0,2,3,4,5,...|\n",
      "|Probability matching|(2000,[0,2,3,4,5,...|\n",
      "|Structural risk m...|(2000,[0,1,2,3,5,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "vecDF = MLUtils.convertVectorColumnsFromML(docTermMatrix, \"tfidfVec\")\n",
    "vecDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aboriginal-complaint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vecDF.select(\"tfidfVec\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "variable-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecRDD = vecDF.select(\"tfidfVec\").rdd.flatMap(lambda x:x)\n",
    "\n",
    "mat = RowMatrix(vecRDD)\n",
    "svd = mat.computeSVD(50, computeU=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "handled-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "termIds = vocabModel.vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "defined-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "docIds = docTermFreqswithID.select(create_map('id', 'title').alias('map'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "innovative-absolute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 map|\n",
      "+--------------------+\n",
      "|[0 -> Bongard pro...|\n",
      "|[1 -> Generative ...|\n",
      "|[2 -> Inductive b...|\n",
      "|[3 -> Category:Ba...|\n",
      "|[4 -> Category:Cl...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docIds.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "meaningful-continuity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.00858180e-02 -4.35775792e-02 -7.34478182e-01 ...  3.56759872e-02\n",
      "   2.37933367e-03 -4.18013106e-02]\n",
      " [-1.20153559e-01 -3.41520254e-02 -3.08447681e-02 ... -1.37015740e-02\n",
      "  -7.39789114e-03  5.90371606e-04]\n",
      " [ 1.85971700e-18  7.50783037e-18 -1.79453700e-19 ...  6.50199192e-18\n",
      "   3.71956703e-19 -3.98281096e-18]\n",
      " ...\n",
      " [-8.78457403e-04 -7.14098445e-04 -3.51210501e-03 ...  1.00068501e-02\n",
      "   7.24102615e-03  1.51146114e-03]\n",
      " [-4.25546919e-03 -1.31132128e-02  2.64652064e-03 ... -1.68947999e-02\n",
      "  -1.44310037e-02 -3.66791509e-02]\n",
      " [-1.78161013e-03 -9.71648805e-05 -1.71628121e-03 ...  5.86748774e-03\n",
      "   1.67048447e-03  4.57107444e-04]]\n"
     ]
    }
   ],
   "source": [
    "v = svd.V\n",
    "arr = v.toArray()\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "respective-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "transposedArr = arr.transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "unavailable-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topTermsInTopConcepts(svd, numConcepts, numTerms, termIds):\n",
    "    arr = svd.V.toArray().transpose()\n",
    "    res = []\n",
    "    for i,v  in enumerate(arr):\n",
    "        if( i > numConcepts ): break\n",
    "\n",
    "        v = list(enumerate(v))\n",
    "        v.sort(key=lambda x : x[1], reverse=True)\n",
    "        v = v[0:numTerms]\n",
    "        v = list((termIds[termId], score) for termId, score in v)\n",
    "        res.append(v)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "african-christopher",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('learn', 1.8597169995295885e-18),\n",
       "  ('machin', -1.6035925690269777e-18),\n",
       "  ('categori', -2.151518488287089e-18),\n",
       "  ('stub', -7.000211758918947e-05),\n",
       "  ('leakag', -0.00018164111877602349),\n",
       "  ('syntaxhighlight', -0.00026571460434930694),\n",
       "  ('automl', -0.00028923376034158035),\n",
       "  ('defaultsort', -0.00029098684759717563),\n",
       "  ('grammat', -0.0003018289912002426),\n",
       "  ('cohen', -0.0003102629786163099)],\n",
       " [('scope', 0.20737308029646853),\n",
       "  ('col', 0.17076117435749236),\n",
       "  ('width', 0.16774931818494626),\n",
       "  ('style', 0.13944109951896713),\n",
       "  ('dataset', 0.08764291274177796),\n",
       "  ('et', 0.07907108650953734),\n",
       "  ('al', 0.07708054580845672),\n",
       "  ('text', 0.045301999677119204),\n",
       "  ('imag', 0.04000949418161115),\n",
       "  ('none', 0.03226686485644428)],\n",
       " [('defn', 0.1512680558263076),\n",
       "  ('gli', 0.0945995509340557),\n",
       "  ('scope', 0.08570611183432925),\n",
       "  ('col', 0.06006244542456554),\n",
       "  ('width', 0.054011334730391075),\n",
       "  ('style', 0.04656215515172694),\n",
       "  ('ghat', 0.02959268003578154),\n",
       "  ('dataset', 0.024496733670962202),\n",
       "  ('term', 0.020517386156565735),\n",
       "  ('yes', 0.01979637788678219)],\n",
       " [('math', 0.12035446676646126),\n",
       "  ('defn', 0.08440393286376088),\n",
       "  ('scope', 0.05551200968539763),\n",
       "  ('gli', 0.052785512582327394),\n",
       "  ('col', 0.039741935288122546),\n",
       "  ('mathbf', 0.03972109659217821),\n",
       "  ('width', 0.038732257712885326),\n",
       "  ('mathcal', 0.036889835499956),\n",
       "  ('style', 0.03418748743436764),\n",
       "  ('x', 0.028324704245546502)],\n",
       " [('quantum', 0.2315360596044999),\n",
       "  ('defn', 0.22515417354363873),\n",
       "  ('math', 0.20228975725189738),\n",
       "  ('scope', 0.15304733820740882),\n",
       "  ('gli', 0.14084361851145225),\n",
       "  ('mathbf', 0.11779818858054367),\n",
       "  ('col', 0.1043623802493719),\n",
       "  ('mathcal', 0.09779095110059333),\n",
       "  ('width', 0.09342344553382659),\n",
       "  ('style', 0.07644497349047717)]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topTermsInTopConcepts(svd, 4, 10, termIds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "responsible-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topDocsInTopConcept(svd, numConcepts, numDocs, docIds):\n",
    "    u = svd.U\n",
    "    res = []\n",
    "\n",
    "    for i, u in enumerate(u.rows.map(lambda i : i.toArray()).collect()):\n",
    "        if( i > numConcepts ): break\n",
    "        u = list(enumerate(u))\n",
    "        u.sort(key=lambda x: x[1], reverse=True)\n",
    "        u = u[0:numDocs]\n",
    "        u = list((docIds.collect()[docId][0][docId], score) for docId, score in u)\n",
    "        res.append(u)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "extended-private",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Uncertain data', 0.012153971612533395),\n",
       "  ('Evolvability (computer science)', 0.011101410429071921),\n",
       "  ('CIML community portal', 0.00883950570724018),\n",
       "  ('Curse of dimensionality', 0.008634068295918167),\n",
       "  ('Concept learning', 0.006993182301313434),\n",
       "  ('Learning with errors', 0.006506466855122496),\n",
       "  ('Probability matching', 0.006330621971353632),\n",
       "  ('Ugly duckling theorem', 0.00459418746538158),\n",
       "  ('Matthews correlation coefficient', 0.004528106337838542),\n",
       "  ('Conditional random field', 0.0043785429594990375)],\n",
       " [('Data pre-processing', 0.05897285911052117),\n",
       "  ('Matthews correlation coefficient', 0.04545270688773978),\n",
       "  ('Learning to rank', 0.0431350496466185),\n",
       "  ('Category:Machine learning researchers', 0.04070487278632756),\n",
       "  ('Prior knowledge for pattern recognition', 0.040071601265590465),\n",
       "  ('Granular computing', 0.035673146974228284),\n",
       "  ('CIML community portal', 0.030504282474660145),\n",
       "  ('Predictive state representation', 0.028612932855883837),\n",
       "  ('Category:Learning in computer vision', 0.02721265731133607),\n",
       "  ('Evolvability (computer science)', 0.02581816576294145)],\n",
       " [('Uniform convergence in probability', 0.018356433280116918),\n",
       "  ('Ugly duckling theorem', 0.017158346365931196),\n",
       "  ('Learning with errors', 0.01422857726997861),\n",
       "  ('Category:Learning in computer vision', 0.013522047090777483),\n",
       "  ('Predictive state representation', 0.010884224561886334),\n",
       "  ('Center for Biological and Computational Learning', 0.0099396233259063),\n",
       "  ('CIML community portal', 0.009856527832767602),\n",
       "  ('Category:Ensemble learning', 0.009762324278326084),\n",
       "  ('Eager learning', 0.009285613084181611),\n",
       "  ('Probability matching', 0.008679169306842639)],\n",
       " [('Category:Ensemble learning', 0.0011468238013988086),\n",
       "  ('CIML community portal', 0.0004482124621339058),\n",
       "  ('Overfitting', 0.00035325684809392726),\n",
       "  ('Neural modeling fields', 0.00031652554171964094),\n",
       "  ('Knowledge integration', 0.0003111156445903527),\n",
       "  ('Learning to rank', 0.0002654950538033083),\n",
       "  ('Prior knowledge for pattern recognition', 0.00025042571026802596),\n",
       "  ('Evolvability (computer science)', 0.0002481833462528634),\n",
       "  ('Transduction (machine learning)', 0.00022341436144686583),\n",
       "  ('Learning automaton', 0.00018789097710742732)],\n",
       " [('Neural modeling fields', 0.0007513902988702331),\n",
       "  ('Matthews correlation coefficient', 0.0006970466348994987),\n",
       "  ('Learning to rank', 0.0005868103050316529),\n",
       "  ('Knowledge integration', 0.0005238659760471336),\n",
       "  ('Expectation propagation', 0.00048574683907414985),\n",
       "  ('Multiple-instance learning', 0.00038986405424075346),\n",
       "  ('Transduction (machine learning)', 0.00038617563827325504),\n",
       "  ('Evolvability (computer science)', 0.00037930596646474236),\n",
       "  ('Semantic analysis (machine learning)', 0.00032205348767300423),\n",
       "  ('Concept drift', 0.0003065307621572611)]]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topDocsInTopConcept(svd, 4, 10, docIds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-vector",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-teach",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-mistress",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-complement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-johnson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-shock",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-antibody",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-irish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-grade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-lounge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "graduate-pottery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|     SnowballStemmed|\n",
      "+--------------------+--------------------+\n",
      "|Category:Unsuperv...|[cat, main, categ...|\n",
      "|Category:Datasets...|[categori, datase...|\n",
      "|Category:Machine ...|[commonscat, cate...|\n",
      "|Category:Ontology...|[categori, ontolo...|\n",
      "|Category:Supervis...|[catmain, categor...|\n",
      "|Category:Semisupe...|[catmain, categor...|\n",
      "|Category:Applied ...|[catmain, machin,...|\n",
      "|Multiple-instance...|[redirect, multip...|\n",
      "|      Validation set|[redirect, train,...|\n",
      "|Category:Inductiv...|[cat, main, categ...|\n",
      "|Category:Structur...|[cat, main, struc...|\n",
      "|Category:Machine ...|[research, studi,...|\n",
      "|Category:Classifi...|[common, categori...|\n",
      "|Category:Computat...|[cat, main, compu...|\n",
      "|Category:Bayesian...|[cat, main, bayes...|\n",
      "|Category:Cluster ...|[common, cat, clu...|\n",
      "|Category:Support ...|[catmain, support...|\n",
      "|Category:Deep lea...|[catmain, deep, l...|\n",
      "|Category:Latent v...|[categori, statis...|\n",
      "|Category:Kernel m...|[common, categori...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "# hashingTF = HashingTF(inputCol=\"SnowballStemmed\", outputCol=\"TF\", numFeatures=20)\n",
    "# featurizedData = hashingTF.transform(stemmed_words)\n",
    "\n",
    "countVectorizer = CountVectorizer(inputCol=\"SnowballStemmed\",\n",
    "                                      outputCol=\"termFreqs\",\n",
    "                                      vocabSize=20000)\n",
    "\n",
    "vocabModel = countVectorizer.fit(stemmed_words)\n",
    "docTermFreqs = vocabModel.transform(stemmed_words)\n",
    "\n",
    "idf = IDF(inputCol=\"TF\", outputCol=\"IDF\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "\n",
    "\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "\n",
    "rescaledData.select(\"title\",\"SnowballStemmed\").orderBy('IDF',ascending=True).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "stuffed-gravity",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '(`IDF` * `TF`)' due to data type mismatch: '(`IDF` * `TF`)' requires numeric type, not struct<type:tinyint,size:int,indices:array<int>,values:array<double>>;;\\n'Project [title#4, SnowballStemmed#244, TF#1160, IDF#1167, (IDF#1167 * TF#1160) AS TF_IDF#1183]\\n+- Project [title#4, SnowballStemmed#244, TF#1160, UDF(TF#1160) AS IDF#1167]\\n   +- Project [title#4, SnowballStemmed#244, UDF(SnowballStemmed#244) AS TF#1160]\\n      +- Project [title#4, SnowballStemmed#244]\\n         +- Project [title#4, refined_text#100, word_stem(refined_text#100) AS SnowballStemmed#244]\\n            +- Project [title#4, refined_text#100]\\n               +- Project [title#4, words_clean#42, remove_empty_word(words_clean#42) AS refined_text#100]\\n                  +- Project [title#4, words_clean#42]\\n                     +- Project [title#4, words_token#36, UDF(words_token#36) AS words_clean#42]\\n                        +- Project [title#4, words_token#36]\\n                           +- Project [title#4, text#26, UDF(text#26) AS words_token#36]\\n                              +- Project [title#4, lower(regexp_replace(raw_text#13, [^a-zA-Z0-9\\\\s ],  )) AS text#26]\\n                                 +- Project [title#4, raw_text#13]\\n                                    +- Project [title#4, _VALUE#10, _VALUE#10 AS raw_text#13]\\n                                       +- Project [title#4, revision#3.text._VALUE AS _VALUE#10]\\n                                          +- Relation[id#0L,ns#1L,redirect#2,revision#3,title#4] XmlRelation(<function0>,Some(wiki_ml.xml),Map(rowtag -> page, path -> wiki_ml.xml),null)\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.7.10/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.10/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4924.withColumn.\n: org.apache.spark.sql.AnalysisException: cannot resolve '(`IDF` * `TF`)' due to data type mismatch: '(`IDF` * `TF`)' requires numeric type, not struct<type:tinyint,size:int,indices:array<int>,values:array<double>>;;\n'Project [title#4, SnowballStemmed#244, TF#1160, IDF#1167, (IDF#1167 * TF#1160) AS TF_IDF#1183]\n+- Project [title#4, SnowballStemmed#244, TF#1160, UDF(TF#1160) AS IDF#1167]\n   +- Project [title#4, SnowballStemmed#244, UDF(SnowballStemmed#244) AS TF#1160]\n      +- Project [title#4, SnowballStemmed#244]\n         +- Project [title#4, refined_text#100, word_stem(refined_text#100) AS SnowballStemmed#244]\n            +- Project [title#4, refined_text#100]\n               +- Project [title#4, words_clean#42, remove_empty_word(words_clean#42) AS refined_text#100]\n                  +- Project [title#4, words_clean#42]\n                     +- Project [title#4, words_token#36, UDF(words_token#36) AS words_clean#42]\n                        +- Project [title#4, words_token#36]\n                           +- Project [title#4, text#26, UDF(text#26) AS words_token#36]\n                              +- Project [title#4, lower(regexp_replace(raw_text#13, [^a-zA-Z0-9\\s ],  )) AS text#26]\n                                 +- Project [title#4, raw_text#13]\n                                    +- Project [title#4, _VALUE#10, _VALUE#10 AS raw_text#13]\n                                       +- Project [title#4, revision#3.text._VALUE AS _VALUE#10]\n                                          +- Relation[id#0L,ns#1L,redirect#2,revision#3,title#4] XmlRelation(<function0>,Some(wiki_ml.xml),Map(rowtag -> page, path -> wiki_ml.xml),null)\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:116)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2258)\n\tat org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2225)\n\tat sun.reflect.GeneratedMethodAccessor87.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-178-d54436d2b8ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrescaledData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TF_IDF'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrescaledData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIDF\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrescaledData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.10/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \"\"\"\n\u001b[1;32m   1997\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.10/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.10/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '(`IDF` * `TF`)' due to data type mismatch: '(`IDF` * `TF`)' requires numeric type, not struct<type:tinyint,size:int,indices:array<int>,values:array<double>>;;\\n'Project [title#4, SnowballStemmed#244, TF#1160, IDF#1167, (IDF#1167 * TF#1160) AS TF_IDF#1183]\\n+- Project [title#4, SnowballStemmed#244, TF#1160, UDF(TF#1160) AS IDF#1167]\\n   +- Project [title#4, SnowballStemmed#244, UDF(SnowballStemmed#244) AS TF#1160]\\n      +- Project [title#4, SnowballStemmed#244]\\n         +- Project [title#4, refined_text#100, word_stem(refined_text#100) AS SnowballStemmed#244]\\n            +- Project [title#4, refined_text#100]\\n               +- Project [title#4, words_clean#42, remove_empty_word(words_clean#42) AS refined_text#100]\\n                  +- Project [title#4, words_clean#42]\\n                     +- Project [title#4, words_token#36, UDF(words_token#36) AS words_clean#42]\\n                        +- Project [title#4, words_token#36]\\n                           +- Project [title#4, text#26, UDF(text#26) AS words_token#36]\\n                              +- Project [title#4, lower(regexp_replace(raw_text#13, [^a-zA-Z0-9\\\\s ],  )) AS text#26]\\n                                 +- Project [title#4, raw_text#13]\\n                                    +- Project [title#4, _VALUE#10, _VALUE#10 AS raw_text#13]\\n                                       +- Project [title#4, revision#3.text._VALUE AS _VALUE#10]\\n                                          +- Relation[id#0L,ns#1L,redirect#2,revision#3,title#4] XmlRelation(<function0>,Some(wiki_ml.xml),Map(rowtag -> page, path -> wiki_ml.xml),null)\\n\""
     ]
    }
   ],
   "source": [
    "rescaledData.withColumn('TF_IDF',rescaledData.IDF * rescaledData.TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "every-detector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- SnowballStemmed: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- TF: vector (nullable = true)\n",
      " |-- IDF: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(DoubleType())\n",
    "def multiplication(TF: pd.Series,IDF: pd.Series) -> pd.Series:\n",
    "    TFIDF = []\n",
    "    return TFIDF.append(DoubleType(a)*DoubleType(b) for a,b in zip(TF,IDF))\n",
    "    \n",
    "rescaledData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "authentic-christopher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------------+--------------------+\n",
      "|           title|     SnowballStemmed|                  TF|                 IDF|\n",
      "+----------------+--------------------+--------------------+--------------------+\n",
      "| Bongard problem|[file, bongard, p...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "|Generative model|[generat, model, ...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|\n",
      "+----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = rescaledData['TF'].cast('double')\n",
    "rescaledData.show(2)\n",
    "\n",
    "# rescaledData_cast = rescaledData.withColumn('value_casted' , rescaledData['TF'].cast('double'))\n",
    "# rescaledData_cast.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "south-implementation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|           title|     SnowballStemmed|                  TF|                 IDF|             TF_cast|            IDF_cast|\n",
      "+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| Bongard problem|[file, bongard, p...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|[15.0,36.0,15.0,1...|[1.85125280061959...|\n",
      "|Generative model|[generat, model, ...|(20,[0,1,2,3,4,5,...|(20,[0,1,2,3,4,5,...|[58.0,70.0,77.0,1...|[7.15817749572909...|\n",
      "+----------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "# assembler = VectorAssembler(inputCols=[\"TF\"],outputCol=\"TF_cast\")\n",
    "# rescaledData = assembler.transform(rescaledData)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"IDF\"],outputCol=\"IDF_cast\")\n",
    "rescaledData = assembler.transform(rescaledData)\n",
    "\n",
    "rescaledData.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "operational-humanity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- SnowballStemmed: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- TF: vector (nullable = true)\n",
      " |-- IDF: vector (nullable = true)\n",
      " |-- TF_cast: vector (nullable = true)\n",
      " |-- IDF_cast: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescaledData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "recorded-october",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'TF'>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col('TF').cast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "authorized-frequency",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-9edb96164f3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrescaledData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TF'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'double'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "rescaledData['TF'].cast('double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "handy-theory",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2989.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 66.0 failed 1 times, most recent failure: Lost task 0.0 in stage 66.0 (TID 65, localhost, executor driver): java.lang.UnsupportedOperationException: Unsupported data type: struct<type:tinyint,size:int,indices:array<int>,values:array<double>>\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$.toArrowType(ArrowUtils.scala:56)\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$.toArrowField(ArrowUtils.scala:92)\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$$anonfun$toArrowSchema$1.apply(ArrowUtils.scala:116)\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$$anonfun$toArrowSchema$1.apply(ArrowUtils.scala:115)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$.toArrowSchema(ArrowUtils.scala:115)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$2.writeIteratorToStream(ArrowPythonRunner.scala:71)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.UnsupportedOperationException: Unsupported data type: struct<type:tinyint,size:int,indices:array<int>,values:array<double>>\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$.toArrowType(ArrowUtils.scala:56)\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$.toArrowField(ArrowUtils.scala:92)\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$$anonfun$toArrowSchema$1.apply(ArrowUtils.scala:116)\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$$anonfun$toArrowSchema$1.apply(ArrowUtils.scala:115)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$.toArrowSchema(ArrowUtils.scala:115)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$2.writeIteratorToStream(ArrowPythonRunner.scala:71)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-10b17a5a2d36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrescaledData_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrescaledData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf_idf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmultiplication\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrescaledData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrescaledData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrescaledData_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.10/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.10/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.10/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.10/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2989.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 66.0 failed 1 times, most recent failure: Lost task 0.0 in stage 66.0 (TID 65, localhost, executor driver): java.lang.UnsupportedOperationException: Unsupported data type: struct<type:tinyint,size:int,indices:array<int>,values:array<double>>\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$.toArrowType(ArrowUtils.scala:56)\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$.toArrowField(ArrowUtils.scala:92)\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$$anonfun$toArrowSchema$1.apply(ArrowUtils.scala:116)\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$$anonfun$toArrowSchema$1.apply(ArrowUtils.scala:115)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$.toArrowSchema(ArrowUtils.scala:115)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$2.writeIteratorToStream(ArrowPythonRunner.scala:71)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.UnsupportedOperationException: Unsupported data type: struct<type:tinyint,size:int,indices:array<int>,values:array<double>>\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$.toArrowType(ArrowUtils.scala:56)\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$.toArrowField(ArrowUtils.scala:92)\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$$anonfun$toArrowSchema$1.apply(ArrowUtils.scala:116)\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$$anonfun$toArrowSchema$1.apply(ArrowUtils.scala:115)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:99)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat org.apache.spark.sql.types.StructType.map(StructType.scala:99)\n\tat org.apache.spark.sql.execution.arrow.ArrowUtils$.toArrowSchema(ArrowUtils.scala:115)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$2.writeIteratorToStream(ArrowPythonRunner.scala:71)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:346)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:195)\n"
     ]
    }
   ],
   "source": [
    "rescaledData_test = rescaledData.withColumn(\"tf_idf\",multiplication(rescaledData.TF,rescaledData.IDF))\n",
    "rescaledData_test.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "entitled-springer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|     SnowballStemmed|\n",
      "+--------------------+--------------------+\n",
      "|List of datasets ...|[use, dmi, date, ...|\n",
      "|Convolutional neu...|[use, cnn, disamb...|\n",
      "|       Mixture model|[distinguish, mix...|\n",
      "|Cross-validation ...|[short, descript,...|\n",
      "|    Machine learning|[short, descript,...|\n",
      "|         Time series|[use, american, e...|\n",
      "|Sparse dictionary...|[br, machin, lear...|\n",
      "|Glossary of artif...|[short, descript,...|\n",
      "| Pattern recognition|[pattern, recogni...|\n",
      "|       Random forest|[machin, learn, t...|\n",
      "|Formal concept an...|[short, descript,...|\n",
      "|            Word2vec|[machin, learn, b...|\n",
      "|Statistical class...|[unsupervis, lear...|\n",
      "|    Algorithmic bias|[short, descript,...|\n",
      "|Quantum machine l...|[short, descript,...|\n",
      "|Dimensionality re...|[short, descript,...|\n",
      "|Matchbox Educable...|[good, articl, us...|\n",
      "|Apprenticeship le...|[machin, learn, v...|\n",
      "|  Granular computing|[granular, comput...|\n",
      "|Conditional rando...|[multipl, issu, c...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "extensive-glossary",
   "metadata": {},
   "source": [
    "참고할 코드가 있음에도 불구하고 이해하는데 꽤나 많은 시간이 필요했다.\n",
    "withColumn의 input이 iterator로 작동하고, 그것을 이용해 UDF를 사용하는데 꽤나 오래걸렸다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
