{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "\n",
    "## Referrence\n",
    "- pandas udf: https://docs.microsoft.com/ko-kr/azure/databricks/spark/latest/spark-sql/udf-python-pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "# from pytz import timezone\n",
    "# from pytz import utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXECUTOR_MEMORY = \"2g\"\n",
    "EXECUTOR_CORES = 2\n",
    "EXECUTORE_INSTANCES = 3\n",
    "DRIVER_MEMORY = \"1g\"\n",
    "DRIVER_MAX_RESULT_SIZE = \"1g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '43429'),\n",
       " ('spark.executor.memory', '2g'),\n",
       " ('spark.app.name', 'Advanced analytics with SPARK - Chapter 4'),\n",
       " ('spark.executor.instances', '3'),\n",
       " ('spark.driver.host', '0b63c6cfbaf6'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.kryoserializer.buffer.max', '1024m'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.maxResultSize', '1g'),\n",
       " ('spark.driver.memory', '1g'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.app.id', 'local-1611562873520'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(f\"Advanced analytics with SPARK - Chapter 4\")\n",
    "    .config(\"spark.executor.memory\", EXECUTOR_MEMORY)\n",
    "    .config(\"spark.executor.cores\", EXECUTOR_CORES)\n",
    "    .config(\"spark.executor.instances\", EXECUTORE_INSTANCES)\n",
    "    .config(\"spark.driver.memory\", DRIVER_MEMORY)\n",
    "    .config(\"spark.driver.maxResultSize\", DRIVER_MAX_RESULT_SIZE)\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\n",
    "#     .config(\"spark.sql.warehouse.dir\", \"/user/bigdata/members/shyeon/advanced-spark/data\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Forest CoverType dataset\n",
      "\n",
      "\n",
      "1.\tTitle of Database:\n",
      "\n",
      "\tForest Covertype data\n",
      "\n",
      "\n",
      "2.\tSources:\n",
      "\n",
      "\t(a) Original owners of database:\n",
      "\t\tRemote Sensing and GIS Program\n",
      "\t\tDepartment of Forest Sciences\n",
      "\t\tCollege of Natural Resources\n",
      "\t\tColorado State University\n",
      "\t\tFort Collins, CO  80523\n",
      "\t\t(contact Jock A. Blackard, jblackard 'at' fs.fed.us\n",
      "\t\t      or Dr. Denis J. Dean, denis.dean 'at' utdallas.edu)\n",
      "\n",
      "\tNOTE:\tReuse of this database is unlimited with retention of \n",
      "\t\tcopyright notice for Jock A. Blackard and Colorado \n",
      "\t\tState University.\n",
      "\n",
      "\t(b) Donors of database:\n",
      "\t\tJock A. Blackard (jblackard 'at' fs.fed.us)\n",
      "\t\tGIS Coordinator\n",
      "\t\tUSFS - Forest Inventory & Analysis\n",
      "\t\tRocky Mountain Research Station\n",
      "\t\t507 25th Street\n",
      "\t\tOgden, UT 84401\n",
      "\n",
      "\t\tDr. Denis J. Dean (denis.dean 'at' utdallas.edu)\n",
      "\t\tProfessor\n",
      "\t\tProgram in Geography and Geospatial Sciences\n",
      "\t\tSchool of Economic, Political and Policy Sciences\n",
      "\t\t800 West Campbell Rd\n",
      "\t\tRichardson, TX  75080-3021 \n",
      "\t\t\n",
      "\t\tDr. Charles W. Anderson (anderson 'at' cs.colostate.edu)\n",
      "\t\tAssociate Professor\n",
      "\t\tDepartment of Computer Science\n",
      "\t\tColorado State University\n",
      "\t\tFort Collins, CO  80523  USA\n",
      "\n",
      "\t(c) Date donated:  August 1998\n",
      "\n",
      "\n",
      "3.\tPast Usage:\n",
      "\n",
      "\tBlackard, Jock A. and Denis J. Dean.  2000.  \"Comparative\n",
      "\t\tAccuracies of Artificial Neural Networks and Discriminant\n",
      "\t\tAnalysis in Predicting Forest Cover Types from Cartographic\n",
      "\t\tVariables.\"  Computers and Electronics in Agriculture \n",
      "\t\t24(3):131-151.\n",
      "\n",
      "\tBlackard, Jock A. and Denis J. Dean.  1998.  \"Comparative\n",
      "\t\tAccuracies of Neural Networks and Discriminant Analysis\n",
      "\t\tin Predicting Forest Cover Types from Cartographic \n",
      "\t\tVariables.\"  Second Southern Forestry GIS Conference.\n",
      "\t\tUniversity of Georgia.  Athens, GA.  Pages 189-199.\n",
      "\n",
      "\tBlackard, Jock A.  1998.  \"Comparison of Neural Networks and\n",
      "\t\tDiscriminant Analysis in Predicting Forest Cover Types.\"\n",
      "\t\tPh.D. dissertation.  Department of Forest Sciences.  \n",
      "\t\tColorado State University.  Fort Collins, Colorado.  \n",
      "\t\t165 pages.\n",
      "\n",
      "\tAbstract of dissertation:\n",
      "\t\tNatural resource managers responsible for developing \n",
      "\tecosystem management strategies require basic descriptive \n",
      "\tinformation including inventory data for forested lands to \n",
      "\tsupport their decision-making processes.  However, managers \n",
      "\tgenerally do not have this type of data for inholdings or \n",
      "\tneighboring lands that are outside their immediate \n",
      "\tjurisdiction.  One method of obtaining this information is \n",
      "\tthrough the use of predictive models.  \n",
      "\t\tTwo predictive models were examined in this study, a \n",
      "\tfeedforward neural network model and a more traditional \n",
      "\tstatistical model based on discriminant analysis.  The overall \n",
      "\tobjectives of this research were to first construct these two \n",
      "\tpredictive models, and second to compare and evaluate their \n",
      "\trespective classification accuracies when predicting forest \n",
      "\tcover types in undisturbed forests.  \n",
      "\t\tThe study area included four wilderness areas found in \n",
      "\tthe Roosevelt National Forest of northern Colorado.  A total \n",
      "\tof twelve cartographic measures were utilized as independent \n",
      "\tvariables in the predictive models, while seven major forest \n",
      "\tcover types were used as dependent variables.  Several subsets \n",
      "\tof these variables were examined to determine the best overall \n",
      "\tpredictive model.  \n",
      "\t\tFor each subset of cartographic variables examined in \n",
      "\tthis study, relative classification accuracies indicate the \n",
      "\tneural network approach outperformed the traditional \n",
      "\tdiscriminant analysis method in predicting forest cover types.  \n",
      "\tThe final neural network model had a higher absolute \n",
      "\tclassification accuracy (70.58%) than the final corresponding \n",
      "\tlinear discriminant analysis model(58.38%).  In support of these \n",
      "\tclassification results, thirty additional networks with randomly \n",
      "\tselected initial weights were derived.  From these networks, the \n",
      "\toverall mean absolute classification accuracy for the neural \n",
      "\tnetwork method was 70.52%, with a 95% confidence interval of \n",
      "\t70.26% to 70.80%.  Consequently, natural resource managers may \n",
      "\tutilize an alternative method of predicting forest cover types \n",
      "\tthat is both superior to the traditional statistical methods and \n",
      "\tadequate to support their decision-making processes for \n",
      "\tdeveloping ecosystem management strategies.\n",
      "\n",
      "\n",
      "\t-- Classification performance\n",
      "\t\t-- first 11,340 records used for training data subset\n",
      "\t\t-- next 3,780 records used for validation data subset\n",
      "\t\t-- last 565,892 records used for testing data subset\n",
      "\t\t-- 70% Neural Network (backpropagation)\n",
      "\t\t-- 58% Linear Discriminant Analysis\n",
      "\n",
      "\n",
      "4.\tRelevant Information Paragraph:\n",
      "\n",
      "\tPredicting forest cover type from cartographic variables only\n",
      "\t(no remotely sensed data).  The actual forest cover type for\n",
      "\ta given observation (30 x 30 meter cell) was determined from\n",
      "\tUS Forest Service (USFS) Region 2 Resource Information System \n",
      "\t(RIS) data.  Independent variables were derived from data\n",
      "\toriginally obtained from US Geological Survey (USGS) and\n",
      "\tUSFS data.  Data is in raw form (not scaled) and contains\n",
      "\tbinary (0 or 1) columns of data for qualitative independent\n",
      "\tvariables (wilderness areas and soil types).\n",
      "\n",
      "\tThis study area includes four wilderness areas located in the\n",
      "\tRoosevelt National Forest of northern Colorado.  These areas\n",
      "\trepresent forests with minimal human-caused disturbances,\n",
      "\tso that existing forest cover types are more a result of \n",
      "\tecological processes rather than forest management practices.\n",
      "\n",
      "\tSome background information for these four wilderness areas:  \n",
      "\tNeota (area 2) probably has the highest mean elevational value of \n",
      "\tthe 4 wilderness areas. Rawah (area 1) and Comanche Peak (area 3) \n",
      "\twould have a lower mean elevational value, while Cache la Poudre \n",
      "\t(area 4) would have the lowest mean elevational value. \n",
      "\n",
      "\tAs for primary major tree species in these areas, Neota would have \n",
      "\tspruce/fir (type 1), while Rawah and Comanche Peak would probably\n",
      "\thave lodgepole pine (type 2) as their primary species, followed by \n",
      "\tspruce/fir and aspen (type 5). Cache la Poudre would tend to have \n",
      "\tPonderosa pine (type 3), Douglas-fir (type 6), and \n",
      "\tcottonwood/willow (type 4).  \n",
      "\n",
      "\tThe Rawah and Comanche Peak areas would tend to be more typical of \n",
      "\tthe overall dataset than either the Neota or Cache la Poudre, due \n",
      "\tto their assortment of tree species and range of predictive \n",
      "\tvariable values (elevation, etc.)  Cache la Poudre would probably \n",
      "\tbe more unique than the others, due to its relatively low \n",
      "\televation range and species composition. \n",
      "\n",
      "\n",
      "5.\tNumber of instances (observations):  581,012\n",
      "\n",
      "\n",
      "6.\tNumber of Attributes:\t12 measures, but 54 columns of data\n",
      "\t\t\t\t(10 quantitative variables, 4 binary\n",
      "\t\t\t\twilderness areas and 40 binary\n",
      "\t\t\t\tsoil type variables)\n",
      "\n",
      "\n",
      "7.\tAttribute information:\n",
      "\n",
      "Given is the attribute name, attribute type, the measurement unit and\n",
      "a brief description.  The forest cover type is the classification \n",
      "problem.  The order of this listing corresponds to the order of \n",
      "numerals along the rows of the database.\n",
      "\n",
      "Name                                     Data Type    Measurement                       Description\n",
      "\n",
      "Elevation                               quantitative    meters                       Elevation in meters\n",
      "Aspect                                  quantitative    azimuth                      Aspect in degrees azimuth\n",
      "Slope                                   quantitative    degrees                      Slope in degrees\n",
      "Horizontal_Distance_To_Hydrology        quantitative    meters                       Horz Dist to nearest surface water features\n",
      "Vertical_Distance_To_Hydrology          quantitative    meters                       Vert Dist to nearest surface water features\n",
      "Horizontal_Distance_To_Roadways         quantitative    meters                       Horz Dist to nearest roadway\n",
      "Hillshade_9am                           quantitative    0 to 255 index               Hillshade index at 9am, summer solstice\n",
      "Hillshade_Noon                          quantitative    0 to 255 index               Hillshade index at noon, summer soltice\n",
      "Hillshade_3pm                           quantitative    0 to 255 index               Hillshade index at 3pm, summer solstice\n",
      "Horizontal_Distance_To_Fire_Points      quantitative    meters                       Horz Dist to nearest wildfire ignition points\n",
      "Wilderness_Area (4 binary columns)      qualitative     0 (absence) or 1 (presence)  Wilderness area designation\n",
      "Soil_Type (40 binary columns)           qualitative     0 (absence) or 1 (presence)  Soil Type designation\n",
      "Cover_Type (7 types)                    integer         1 to 7                       Forest Cover Type designation\n",
      "\n",
      "\n",
      "Code Designations:\n",
      "\n",
      "Wilderness Areas:  \t1 -- Rawah Wilderness Area\n",
      "                        2 -- Neota Wilderness Area\n",
      "                        3 -- Comanche Peak Wilderness Area\n",
      "                        4 -- Cache la Poudre Wilderness Area\n",
      "\n",
      "Soil Types:             1 to 40 : based on the USFS Ecological\n",
      "                        Landtype Units (ELUs) for this study area:\n",
      "\n",
      "  Study Code USFS ELU Code\t\t\tDescription\n",
      "\t 1\t   2702\t\tCathedral family - Rock outcrop complex, extremely stony.\n",
      "\t 2\t   2703\t\tVanet - Ratake families complex, very stony.\n",
      "\t 3\t   2704\t\tHaploborolis - Rock outcrop complex, rubbly.\n",
      "\t 4\t   2705\t\tRatake family - Rock outcrop complex, rubbly.\n",
      "\t 5\t   2706\t\tVanet family - Rock outcrop complex complex, rubbly.\n",
      "\t 6\t   2717\t\tVanet - Wetmore families - Rock outcrop complex, stony.\n",
      "\t 7\t   3501\t\tGothic family.\n",
      "\t 8\t   3502\t\tSupervisor - Limber families complex.\n",
      "\t 9\t   4201\t\tTroutville family, very stony.\n",
      "\t10\t   4703\t\tBullwark - Catamount families - Rock outcrop complex, rubbly.\n",
      "\t11\t   4704\t\tBullwark - Catamount families - Rock land complex, rubbly.\n",
      "\t12\t   4744\t\tLegault family - Rock land complex, stony.\n",
      "\t13\t   4758\t\tCatamount family - Rock land - Bullwark family complex, rubbly.\n",
      "\t14\t   5101\t\tPachic Argiborolis - Aquolis complex.\n",
      "\t15\t   5151\t\tunspecified in the USFS Soil and ELU Survey.\n",
      "\t16\t   6101\t\tCryaquolis - Cryoborolis complex.\n",
      "\t17\t   6102\t\tGateview family - Cryaquolis complex.\n",
      "\t18\t   6731\t\tRogert family, very stony.\n",
      "\t19\t   7101\t\tTypic Cryaquolis - Borohemists complex.\n",
      "\t20\t   7102\t\tTypic Cryaquepts - Typic Cryaquolls complex.\n",
      "\t21\t   7103\t\tTypic Cryaquolls - Leighcan family, till substratum complex.\n",
      "\t22\t   7201\t\tLeighcan family, till substratum, extremely bouldery.\n",
      "\t23\t   7202\t\tLeighcan family, till substratum - Typic Cryaquolls complex.\n",
      "\t24\t   7700\t\tLeighcan family, extremely stony.\n",
      "\t25\t   7701\t\tLeighcan family, warm, extremely stony.\n",
      "\t26\t   7702\t\tGranile - Catamount families complex, very stony.\n",
      "\t27\t   7709\t\tLeighcan family, warm - Rock outcrop complex, extremely stony.\n",
      "\t28\t   7710\t\tLeighcan family - Rock outcrop complex, extremely stony.\n",
      "\t29\t   7745\t\tComo - Legault families complex, extremely stony.\n",
      "\t30\t   7746\t\tComo family - Rock land - Legault family complex, extremely stony.\n",
      "\t31\t   7755\t\tLeighcan - Catamount families complex, extremely stony.\n",
      "\t32\t   7756\t\tCatamount family - Rock outcrop - Leighcan family complex, extremely stony.\n",
      "\t33\t   7757\t\tLeighcan - Catamount families - Rock outcrop complex, extremely stony.\n",
      "\t34\t   7790\t\tCryorthents - Rock land complex, extremely stony.\n",
      "\t35\t   8703\t\tCryumbrepts - Rock outcrop - Cryaquepts complex.\n",
      "\t36\t   8707\t\tBross family - Rock land - Cryumbrepts complex, extremely stony.\n",
      "\t37\t   8708\t\tRock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\n",
      "\t38\t   8771\t\tLeighcan - Moran families - Cryaquolls complex, extremely stony.\n",
      "\t39\t   8772\t\tMoran family - Cryorthents - Leighcan family complex, extremely stony.\n",
      "\t40\t   8776\t\tMoran family - Cryorthents - Rock land complex, extremely stony.\n",
      "\n",
      "        Note:   First digit:  climatic zone             Second digit:  geologic zones\n",
      "                1.  lower montane dry                   1.  alluvium\n",
      "                2.  lower montane                       2.  glacial\n",
      "                3.  montane dry                         3.  shale\n",
      "                4.  montane                             4.  sandstone\n",
      "                5.  montane dry and montane             5.  mixed sedimentary\n",
      "                6.  montane and subalpine               6.  unspecified in the USFS ELU Survey\n",
      "                7.  subalpine                           7.  igneous and metamorphic\n",
      "                8.  alpine                              8.  volcanic\n",
      "\n",
      "        The third and fourth ELU digits are unique to the mapping unit \n",
      "        and have no special meaning to the climatic or geologic zones.\n",
      "\n",
      "Forest Cover Type Classes:\t1 -- Spruce/Fir\n",
      "                                2 -- Lodgepole Pine\n",
      "                                3 -- Ponderosa Pine\n",
      "                                4 -- Cottonwood/Willow\n",
      "                                5 -- Aspen\n",
      "                                6 -- Douglas-fir\n",
      "                                7 -- Krummholz\n",
      "\n",
      "\n",
      "8.  Basic Summary Statistics for quantitative variables only\n",
      "\t(whole dataset -- thanks to Phil Rennert for the summary values):\n",
      "\n",
      "Name                                    Units             Mean   Std Dev\n",
      "Elevation                               meters          2959.36  279.98\n",
      "Aspect                                  azimuth          155.65  111.91\n",
      "Slope                                   degrees           14.10    7.49\n",
      "Horizontal_Distance_To_Hydrology        meters           269.43  212.55\n",
      "Vertical_Distance_To_Hydrology          meters            46.42   58.30\n",
      "Horizontal_Distance_To_Roadways         meters          2350.15 1559.25\n",
      "Hillshade_9am                           0 to 255 index   212.15   26.77\n",
      "Hillshade_Noon                          0 to 255 index   223.32   19.77\n",
      "Hillshade_3pm                           0 to 255 index   142.53   38.27\n",
      "Horizontal_Distance_To_Fire_Points      meters          1980.29 1324.19\n",
      "\n",
      "\n",
      "9.\tMissing Attribute Values:  None.\n",
      "\n",
      "\n",
      "10.\tClass distribution:\n",
      "\n",
      "           Number of records of Spruce-Fir:                211840 \n",
      "           Number of records of Lodgepole Pine:            283301 \n",
      "           Number of records of Ponderosa Pine:             35754 \n",
      "           Number of records of Cottonwood/Willow:           2747 \n",
      "           Number of records of Aspen:                       9493 \n",
      "           Number of records of Douglas-fir:                17367 \n",
      "           Number of records of Krummholz:                  20510  \n",
      "           Number of records of other:                          0  \n",
      "\t\t\n",
      "           Total records:                                  581012\n",
      "\n",
      "=====================================================================\n",
      "Jock A. Blackard\n",
      "08/28/1998 -- original text\n",
      "12/07/1999 -- updated mailing address, citations, background info \n",
      "\t\t  for study area, added summary statistics.\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat /home/jovyan/work/ch04/data/covtype.info\n",
    "ref_path = '/home/jovyan/work/ch04/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2596,51,3,258,0,510,221,232,148,6279,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\n",
      "2590,56,2,212,-6,390,220,235,151,6225,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\n",
      "2804,139,9,268,65,3180,234,238,135,6121,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2\n",
      "2785,155,18,242,118,3090,238,238,122,6211,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,2\n",
      "2595,45,2,153,-1,391,220,234,150,6172,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\n",
      "2579,132,6,300,-15,67,230,237,140,6031,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,2\n",
      "2606,45,7,270,5,633,222,225,138,6256,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\n",
      "2605,49,4,234,7,573,222,230,144,6228,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\n",
      "2617,45,9,240,56,666,223,221,133,6244,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\n",
      "2612,59,10,247,11,636,228,219,124,6230,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5\n"
     ]
    }
   ],
   "source": [
    "!head /home/jovyan/work/ch04/data/covtype.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elevation                               quantitative    meters                       Elevation in meters\n",
    "# Aspect                                  quantitative    azimuth                      Aspect in degrees azimuth\n",
    "# Slope                                   quantitative    degrees                      Slope in degrees\n",
    "# Horizontal_Distance_To_Hydrology        quantitative    meters                       Horz Dist to nearest surface water features\n",
    "# Vertical_Distance_To_Hydrology          quantitative    meters                       Vert Dist to nearest surface water features\n",
    "# Horizontal_Distance_To_Roadways         quantitative    meters                       Horz Dist to nearest roadway\n",
    "# Hillshade_9am                           quantitative    0 to 255 index               Hillshade index at 9am, summer solstice\n",
    "# Hillshade_Noon                          quantitative    0 to 255 index               Hillshade index at noon, summer soltice\n",
    "# Hillshade_3pm                           quantitative    0 to 255 index               Hillshade index at 3pm, summer solstice\n",
    "# Horizontal_Distance_To_Fire_Points      quantitative    meters                       Horz Dist to nearest wildfire ignition points\n",
    "# Wilderness_Area (4 binary columns)      qualitative     0 (absence) or 1 (presence)  Wilderness area designation\n",
    "# Soil_Type (40 binary columns)           qualitative     0 (absence) or 1 (presence)  Soil Type designation\n",
    "# Cover_Type (7 types)                    integer         1 to 7                       Forest Cover Type designation\n",
    "\n",
    "wilderness_area_cols = [f\"wilderness_area_{i}\" for i in range(4)] # 황야 지역 (4 dummy variables)\n",
    "soil_type_cols = [f\"soil_type_{i}\" for i in range(40)] # 토양 유형 (40 dummy variables)\n",
    "# 이 문법 기억하기!\n",
    "\n",
    "schema = [\n",
    "    T.StructField(\"elevation\", T.DoubleType(), True),\n",
    "    T.StructField(\"aspect\", T.DoubleType(), True),\n",
    "    T.StructField(\"slope\", T.DoubleType(), True),\n",
    "    T.StructField(\"horz_dist_to_hydro\", T.DoubleType(), True), # 가장 가까운 지표수까지 거리\n",
    "    T.StructField(\"vert_dist_to_hydro\", T.DoubleType(), True), # 가장 가까운 지표수까지 거리\n",
    "    T.StructField(\"horz_dist_to_road\", T.DoubleType(), True), # 가장 가까운 도로까지 거리\n",
    "    T.StructField(\"hillshade_9am\", T.IntegerType(), True), # 언덕 그늘\n",
    "    T.StructField(\"hillshade_noon\", T.IntegerType(), True),\n",
    "    T.StructField(\"hillshade_3pm\", T.IntegerType(), True),\n",
    "    T.StructField(\"horz_dist_to_fire\", T.DoubleType(), True),\n",
    "]\n",
    "\n",
    "wilderness_area_schema = [T.StructField(col, T.IntegerType(), True) for col in wilderness_area_cols] \n",
    "soil_type_schema = [T.StructField(col, T.IntegerType(), True) for col in soil_type_cols] \n",
    "cover_type_schema = [T.StructField(\"cover_type\", T.IntegerType(), True)]\n",
    "\n",
    "#wilderness_area_schema 너무길어서 따로 만들어 합침. \n",
    "\n",
    "schema.extend(wilderness_area_schema)\n",
    "schema.extend(soil_type_schema)\n",
    "schema.extend(cover_type_schema)\n",
    "schema = T.StructType(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|cover_type|\n",
      "+----------+\n",
      "|         5|\n",
      "|         5|\n",
      "|         2|\n",
      "|         2|\n",
      "|         5|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (spark\n",
    "      .read.format(\"csv\")\n",
    "      .option(\"header\", False)\n",
    "      .option(\"sep\", \",\")\n",
    "      .schema(schema)\n",
    "      .load(ref_path + \"data/covtype.data\"))\n",
    "      \n",
    "\n",
    "wilderness_area_cols = [f\"wilderness_area_{i}\" for i in range(4)]\n",
    "soil_type_cols = [f\"soil_type_{i}\" for i in range(40)]\n",
    "\n",
    "df.select(\"cover_type\").show(5) # 모든 컬럼의 스키마가 반영되었는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of column: 55\n",
      "+--------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                |label|\n",
      "+--------------------------------------------------------------------------------------------------------+-----+\n",
      "|(54,[0,1,2,3,5,6,7,8,9,10,42],[2596.0,51.0,3.0,258.0,510.0,221.0,232.0,148.0,6279.0,1.0,1.0])           |5.0  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,10,42],[2590.0,56.0,2.0,212.0,-6.0,390.0,220.0,235.0,151.0,6225.0,1.0,1.0])    |5.0  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,10,25],[2804.0,139.0,9.0,268.0,65.0,3180.0,234.0,238.0,135.0,6121.0,1.0,1.0])  |2.0  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,10,43],[2785.0,155.0,18.0,242.0,118.0,3090.0,238.0,238.0,122.0,6211.0,1.0,1.0])|2.0  |\n",
      "|(54,[0,1,2,3,4,5,6,7,8,9,10,42],[2595.0,45.0,2.0,153.0,-1.0,391.0,220.0,234.0,150.0,6172.0,1.0,1.0])    |5.0  |\n",
      "+--------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "print(f\"the number of column: {len(df.columns)}\")\n",
    "\n",
    "transformer = RFormula(formula=\"cover_type ~ .\").fit(df)\n",
    "## 질문해야됨!!\n",
    "\n",
    "prepared_df = transformer.transform(df).select(\"features\", \"label\")\n",
    "prepared_df.show(5, False) # features와 label이 추가됨\n",
    "\n",
    "train_df, test_df = prepared_df.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def build_pipeline(ml_model):\n",
    "    # Index labels, adding metadata to the label column.\n",
    "    # Fit on whole dataset to include all labels in index.\n",
    "    label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(prepared_df)\n",
    "\n",
    "    # Automatically identify categorical features, and index them.\n",
    "    # We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "    feature_indexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=8).fit(prepared_df)\n",
    "\n",
    "    # Chain indexers and tree in a Pipeline\n",
    "    stages = [label_indexer, feature_indexer, ml_model]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    return pipeline\n",
    "\n",
    "def predict(train_df, test_df, pipeline, summarize_result=False, summarize_model=False):\n",
    "    model = pipeline.fit(train_df)\n",
    "    # Make predictions.\n",
    "    predictions = model.transform(test_df)\n",
    "    if summarize_result:\n",
    "        predictions.select(\"indexedLabel\", \"prediction\", \"probability\").show(5, False)\n",
    "    if summarize_model:\n",
    "        treeModel = model.stages[2]\n",
    "        print(treeModel)\n",
    "    return predictions\n",
    "\n",
    "def evaluate(predictions):\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\",\n",
    "                                                  predictionCol=\"prediction\",\n",
    "                                                  metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree (Spark Documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|indexedLabel|prediction|probability                                                                                                                   |\n",
      "+------------+----------+------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0         |0.0       |[0.917094017094017,0.03632478632478633,0.0,0.0,0.0,0.04658119658119658,0.0]                                                   |\n",
      "|0.0         |0.0       |[0.777401916890525,0.14035087719298245,0.015247239080688488,3.316419593406958E-5,0.019649786090936226,0.04731701654893377,0.0]|\n",
      "|0.0         |0.0       |[0.777401916890525,0.14035087719298245,0.015247239080688488,3.316419593406958E-5,0.019649786090936226,0.04731701654893377,0.0]|\n",
      "|0.0         |0.0       |[0.777401916890525,0.14035087719298245,0.015247239080688488,3.316419593406958E-5,0.019649786090936226,0.04731701654893377,0.0]|\n",
      "|0.0         |0.0       |[0.777401916890525,0.14035087719298245,0.015247239080688488,3.316419593406958E-5,0.019649786090936226,0.04731701654893377,0.0]|\n",
      "+------------+----------+------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_83e630d42475, depth=5, numNodes=37, numClasses=7, numFeatures=54\n",
      "Test Error = 0.298203 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7017968488871985"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\",\n",
    "                            featuresCol=\"indexedFeatures\",\n",
    "                            predictionCol=\"prediction\",\n",
    "                            seed=42)\n",
    "#print(dt.explainParams())\n",
    "\n",
    "pipeline = build_pipeline(dt)\n",
    "predictions = predict(train_df, test_df, pipeline, True, True)\n",
    "evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "#### Spark API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64968.0</td>\n",
       "      <td>16608.0</td>\n",
       "      <td>2592.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18337.0</td>\n",
       "      <td>43329.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>1633.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>742.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9750.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79.0</td>\n",
       "      <td>2643.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3415.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1078.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4011.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2559.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>318.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1       2       3    4    5      6\n",
       "0  64968.0  16608.0  2592.0   225.0  0.0  0.0   24.0\n",
       "1  18337.0  43329.0    92.0  1633.0  0.0  0.0    0.0\n",
       "2    742.0      0.0  9750.0     0.0  0.0  0.0  199.0\n",
       "3     79.0   2643.0    11.0  3415.0  0.0  0.0    0.0\n",
       "4   1078.0      0.0  4011.0     0.0  0.0  0.0  144.0\n",
       "5   2559.0     24.0   249.0     0.0  0.0  0.0    0.0\n",
       "6      0.0      0.0   496.0     0.0  0.0  0.0  318.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import pandas as pd\n",
    "\n",
    "# important: need to cast to float type, and order by prediction, else it won't work\n",
    "# select only prediction and label columns\n",
    "preds_and_labels = predictions.select('prediction','indexedLabel').orderBy('prediction') # 컬럼순서 주의(pred, real 순) \n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "cm = metrics.confusionMatrix().toArray() # python list\n",
    "pd.DataFrame(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indexedLabel</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>64968</td>\n",
       "      <td>16608</td>\n",
       "      <td>2592</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>18337</td>\n",
       "      <td>43329</td>\n",
       "      <td>92</td>\n",
       "      <td>1633</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>742</td>\n",
       "      <td>0</td>\n",
       "      <td>9750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>79</td>\n",
       "      <td>2643</td>\n",
       "      <td>11</td>\n",
       "      <td>3415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>1078</td>\n",
       "      <td>0</td>\n",
       "      <td>4011</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>2559</td>\n",
       "      <td>24</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0      1     2     3  4  5    6\n",
       "indexedLabel                                     \n",
       "0.0           64968  16608  2592   225  0  0   24\n",
       "1.0           18337  43329    92  1633  0  0    0\n",
       "2.0             742      0  9750     0  0  0  199\n",
       "3.0              79   2643    11  3415  0  0    0\n",
       "4.0            1078      0  4011     0  0  0  144\n",
       "5.0            2559     24   249     0  0  0    0\n",
       "6.0               0      0   496     0  0  0  318"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = (predictions.select('prediction', 'indexedLabel')\n",
    "      .groupby(\"indexedLabel\")\n",
    "      .pivot(\"prediction\", list(range(0, 7)))\n",
    "      .count()\n",
    "      .fillna(0)\n",
    "      .sort(\"indexedLabel\")\n",
    "      .toPandas())\n",
    "cm = cm.reindex(cm[\"indexedLabel\"], axis=0).drop(\"indexedLabel\", axis=1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o717.fit.\n: org.apache.spark.SparkException: Job 31 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:979)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:977)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:977)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2257)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2170)\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:1973)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1973)\n\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1922)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.$anonfun$train$1(DecisionTreeClassifier.scala:135)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:114)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:150)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-13ea4ad04a1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mvalidator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainValidationSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimatorParamMaps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallelism\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mvalidated_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mThreadPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumModels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnumModels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mThreadPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParallelism\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumModels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnumModels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36msingleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msingleTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModel\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No models remaining.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitSingleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfitSingleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamMaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_FitMultipleIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitSingleModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparamMaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[1;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o717.fit.\n: org.apache.spark.SparkException: Job 31 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:979)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:977)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:977)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2257)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2170)\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:1973)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:1973)\n\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1922)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$collectAsMap$1(PairRDDFunctions.scala:737)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:736)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:663)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.runBagged(RandomForest.scala:208)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:302)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.$anonfun$train$1(DecisionTreeClassifier.scala:135)\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:114)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:150)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 58864)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.8/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(dt.impurity, [\"entropy\"])\n",
    "              .addGrid(dt.maxDepth, [30]) # must <= 30\n",
    "              .addGrid(dt.maxBins, [470])\n",
    "              .addGrid(dt.minInfoGain, [0.0])\n",
    "              .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\",\n",
    "                                              predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "\n",
    "validator = TrainValidationSplit(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, parallelism=4, seed=42)\n",
    "validated_model = validator.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='DecisionTreeClassifier_3f5fdea9e4a0', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False,\n",
       " Param(parent='DecisionTreeClassifier_3f5fdea9e4a0', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10,\n",
       " Param(parent='DecisionTreeClassifier_3f5fdea9e4a0', name='featuresCol', doc='features column name.'): 'indexedFeatures',\n",
       " Param(parent='DecisionTreeClassifier_3f5fdea9e4a0', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'entropy',\n",
       " Param(parent='DecisionTreeClassifier_3f5fdea9e4a0', name='labelCol', doc='label column name.'): 'indexedLabel',\n",
       " Param(parent='DecisionTreeClassifier_3f5fdea9e4a0', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '',\n",
       " Param(parent='DecisionTreeClassifier_3f5fdea9e4a0', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 470,\n",
       " Param(parent='DecisionTreeClassifier_3f5fdea9e4a0', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 30,\n",
       " Param(parent='DecisionTreeClassifier_3f5fdea9e4a0', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256,\n",
       " Param(parent='DecisionTreeClassifier_3f5fdea9e4a0', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
       " Param(parent='DecisionTreeClassifier_3f5fdea9e4a0', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\n",
       " Param(parent='DecisionTreeClassifier_3f5fdea9e4a0', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0,\n",
       " Param(parent='DecisionTreeClassifier_3f5fdea9e4a0', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='DecisionTreeClassifier_3f5fdea9e4a0', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
       " Param(parent='DecisionTreeClassifier_3f5fdea9e4a0', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent='DecisionTreeClassifier_3f5fdea9e4a0', name='seed', doc='random seed.'): 42}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validated_model.bestModel.stages[-1].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0644591 \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indexedLabel</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>80172</td>\n",
       "      <td>3747</td>\n",
       "      <td>224</td>\n",
       "      <td>53</td>\n",
       "      <td>174</td>\n",
       "      <td>356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>3840</td>\n",
       "      <td>59434</td>\n",
       "      <td>7</td>\n",
       "      <td>289</td>\n",
       "      <td>6</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>223</td>\n",
       "      <td>2</td>\n",
       "      <td>10029</td>\n",
       "      <td>0</td>\n",
       "      <td>410</td>\n",
       "      <td>29</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>55</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>5726</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>200</td>\n",
       "      <td>22</td>\n",
       "      <td>453</td>\n",
       "      <td>0</td>\n",
       "      <td>4509</td>\n",
       "      <td>8</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>416</td>\n",
       "      <td>46</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2381</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0      1      2     3     4     5    6\n",
       "indexedLabel                                            \n",
       "0.0           80172   3747    224    53   174   356    0\n",
       "1.0            3840  59434      7   289     6    57    0\n",
       "2.0             223      2  10029     0   410    29   91\n",
       "3.0              55    300      0  5726     0     3    0\n",
       "4.0             200     22    453     0  4509     8   37\n",
       "5.0             416     46     26     0    19  2381    0\n",
       "6.0               0      0     89     0    44     0  680"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = validated_model.bestModel.transform(test_df)\n",
    "evaluate(predictions)\n",
    "\n",
    "cm = (predictions.select('prediction', 'indexedLabel')\n",
    "      .groupby(\"indexedLabel\")\n",
    "      .pivot(\"prediction\", list(range(0, 7)))\n",
    "      .count()\n",
    "      .fillna(0)\n",
    "      .sort(\"indexedLabel\")\n",
    "      .toPandas())\n",
    "cm = cm.reindex(cm[\"indexedLabel\"], axis=0).drop(\"indexedLabel\", axis=1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "- onehot으로 분리된 열들을 한 열로 통합\n",
    "- pandas udf를 사용함\n",
    "- soil_type은 indexer 사용 불가(카디널리티가 30 이하여야함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of column: 52\n",
      "+---------------+\n",
      "|wilderness_area|\n",
      "+---------------+\n",
      "|   [1, 0, 0, 0]|\n",
      "|   [1, 0, 0, 0]|\n",
      "|   [1, 0, 0, 0]|\n",
      "|   [1, 0, 0, 0]|\n",
      "|   [1, 0, 0, 0]|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = [col for col in df.columns if (col.find(\"wilderness_area_\") == -1)] \n",
    "new_df = df.select(*cols, F.array(*wilderness_area_cols).alias(\"wilderness_area\"))\n",
    "\n",
    "print(f\"the number of column: {len(new_df.columns)}\")\n",
    "new_df.select(\"wilderness_area\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                         |label|\n",
      "+-------------------------------------------------------------------------------------------------+-----+\n",
      "|(51,[0,1,2,3,5,6,7,8,9,38],[2596.0,51.0,3.0,258.0,510.0,221.0,232.0,148.0,6279.0,1.0])           |5.0  |\n",
      "|(51,[0,1,2,3,4,5,6,7,8,9,38],[2590.0,56.0,2.0,212.0,-6.0,390.0,220.0,235.0,151.0,6225.0,1.0])    |5.0  |\n",
      "|(51,[0,1,2,3,4,5,6,7,8,9,21],[2804.0,139.0,9.0,268.0,65.0,3180.0,234.0,238.0,135.0,6121.0,1.0])  |2.0  |\n",
      "|(51,[0,1,2,3,4,5,6,7,8,9,39],[2785.0,155.0,18.0,242.0,118.0,3090.0,238.0,238.0,122.0,6211.0,1.0])|2.0  |\n",
      "|(51,[0,1,2,3,4,5,6,7,8,9,38],[2595.0,45.0,2.0,153.0,-1.0,391.0,220.0,234.0,150.0,6172.0,1.0])    |5.0  |\n",
      "+-------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: double]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "@F.pandas_udf(T.LongType())\n",
    "def unhot_udf(arrs: pd.Series) -> pd.Series:\n",
    "    return pd.Series(np.where(arr==1)[0][0] for arr in arrs)\n",
    "\n",
    "# Execute function as a Spark vectorized UDF\n",
    "new_df = new_df.select(*cols, unhot_udf(F.col(\"wilderness_area\")).alias(\"wilderness_area\"))\n",
    "transformer = RFormula(formula=\"cover_type ~ .\").fit(new_df)\n",
    "prepared_df = transformer.transform(new_df).select(\"features\", \"label\")\n",
    "prepared_df.show(5, False)\n",
    "\n",
    "train_df, test_df = prepared_df.randomSplit([0.7, 0.3], seed=42)\n",
    "train_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\",\n",
    "                            featuresCol=\"indexedFeatures\",\n",
    "                            predictionCol=\"prediction\",\n",
    "                            numTrees=100)\n",
    "pipeline = build_pipeline(rf)\n",
    "\n",
    "param_grid = (ParamGridBuilder()\n",
    "              .addGrid(dt.maxDepth, [1, 10, 20]) # must <= 30\n",
    "              .addGrid(dt.maxBins, [10, 20, 30, 40, 50])\n",
    "              .addGrid(dt.minInfoGain, [0.0, 0.05])\n",
    "              .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\",\n",
    "                                              predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "\n",
    "validator = TrainValidationSplit(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, parallelism=1, seed=42)\n",
    "validated_model = validator.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_ba60fe650fb8', name='bootstrap', doc='Whether bootstrap samples are used when building trees.'): True,\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False,\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10,\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='featureSubsetStrategy', doc=\"The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto'\"): 'auto',\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='featuresCol', doc='features column name.'): 'indexedFeatures',\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'gini',\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='labelCol', doc='label column name.'): 'indexedLabel',\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '',\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 32,\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 5,\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256,\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0,\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1,\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0,\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='numTrees', doc='Number of trees to train (>= 1).'): 100,\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='seed', doc='random seed.'): -6894470631950539095,\n",
       " Param(parent='RandomForestClassifier_ba60fe650fb8', name='subsamplingRate', doc='Fraction of the training data used for learning each decision tree, in range (0, 1].'): 1.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validated_model.bestModel.stages[2].extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6724794294802965"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = validated_model.bestModel.transform(test_df)\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indexedLabel</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>63756</td>\n",
       "      <td>20307</td>\n",
       "      <td>837</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>16782</td>\n",
       "      <td>46768</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>4079</td>\n",
       "      <td>0</td>\n",
       "      <td>6593</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>28</td>\n",
       "      <td>6120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>2298</td>\n",
       "      <td>0</td>\n",
       "      <td>2936</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>2848</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>805</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0      1     2  3  4  5  6\n",
       "indexedLabel                                \n",
       "0.0           63756  20307   837  0  0  0  0\n",
       "1.0           16782  46768     0  0  0  0  0\n",
       "2.0            4079      0  6593  0  0  0  0\n",
       "3.0              28   6120     0  0  0  0  0\n",
       "4.0            2298      0  2936  0  0  0  0\n",
       "5.0            2848      0     0  0  0  0  0\n",
       "6.0               0      0   805  0  0  0  0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = (predictions.select('prediction', 'indexedLabel')\n",
    "      .groupby(\"indexedLabel\")\n",
    "      .pivot(\"prediction\", list(range(0, 7)))\n",
    "      .count()\n",
    "      .fillna(0)\n",
    "      .sort(\"indexedLabel\")\n",
    "      .toPandas())\n",
    "cm = cm.reindex(cm[\"indexedLabel\"], axis=0).drop(\"indexedLabel\", axis=1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>elevation</td>\n",
       "      <td>0.389976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>cover_type</td>\n",
       "      <td>0.188755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>soil_type_9</td>\n",
       "      <td>0.068208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>soil_type_21</td>\n",
       "      <td>0.063238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>soil_type_11</td>\n",
       "      <td>0.046120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>soil_type_3</td>\n",
       "      <td>0.040963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>soil_type_37</td>\n",
       "      <td>0.030435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>horz_dist_to_road</td>\n",
       "      <td>0.023659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>soil_type_22</td>\n",
       "      <td>0.023002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>soil_type_38</td>\n",
       "      <td>0.020977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>soil_type_1</td>\n",
       "      <td>0.020692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>horz_dist_to_fire</td>\n",
       "      <td>0.019589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>soil_type_5</td>\n",
       "      <td>0.012061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>soil_type_39</td>\n",
       "      <td>0.006948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>slope</td>\n",
       "      <td>0.006745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>soil_type_28</td>\n",
       "      <td>0.005238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>horz_dist_to_hydro</td>\n",
       "      <td>0.004976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hillshade_noon</td>\n",
       "      <td>0.004722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hillshade_9am</td>\n",
       "      <td>0.004299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aspect</td>\n",
       "      <td>0.003177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>soil_type_2</td>\n",
       "      <td>0.002707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>soil_type_12</td>\n",
       "      <td>0.002553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hillshade_3pm</td>\n",
       "      <td>0.002431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vert_dist_to_hydro</td>\n",
       "      <td>0.002378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>soil_type_0</td>\n",
       "      <td>0.002299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>soil_type_29</td>\n",
       "      <td>0.001036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>soil_type_10</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>soil_type_32</td>\n",
       "      <td>0.000605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>soil_type_31</td>\n",
       "      <td>0.000586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>soil_type_30</td>\n",
       "      <td>0.000241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>soil_type_17</td>\n",
       "      <td>0.000191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>soil_type_16</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>soil_type_34</td>\n",
       "      <td>0.000085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>soil_type_36</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>soil_type_23</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>soil_type_20</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>soil_type_4</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>soil_type_13</td>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>soil_type_15</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>soil_type_19</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>soil_type_18</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>soil_type_14</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>soil_type_27</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>soil_type_8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>soil_type_33</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>soil_type_26</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>soil_type_35</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>soil_type_25</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>soil_type_7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>soil_type_6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>soil_type_24</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               feature  importance\n",
       "0            elevation    0.389976\n",
       "50          cover_type    0.188755\n",
       "19         soil_type_9    0.068208\n",
       "31        soil_type_21    0.063238\n",
       "21        soil_type_11    0.046120\n",
       "13         soil_type_3    0.040963\n",
       "47        soil_type_37    0.030435\n",
       "5    horz_dist_to_road    0.023659\n",
       "32        soil_type_22    0.023002\n",
       "48        soil_type_38    0.020977\n",
       "11         soil_type_1    0.020692\n",
       "9    horz_dist_to_fire    0.019589\n",
       "15         soil_type_5    0.012061\n",
       "49        soil_type_39    0.006948\n",
       "2                slope    0.006745\n",
       "38        soil_type_28    0.005238\n",
       "3   horz_dist_to_hydro    0.004976\n",
       "7       hillshade_noon    0.004722\n",
       "6        hillshade_9am    0.004299\n",
       "1               aspect    0.003177\n",
       "12         soil_type_2    0.002707\n",
       "22        soil_type_12    0.002553\n",
       "8        hillshade_3pm    0.002431\n",
       "4   vert_dist_to_hydro    0.002378\n",
       "10         soil_type_0    0.002299\n",
       "39        soil_type_29    0.001036\n",
       "20        soil_type_10    0.000800\n",
       "42        soil_type_32    0.000605\n",
       "41        soil_type_31    0.000586\n",
       "40        soil_type_30    0.000241\n",
       "27        soil_type_17    0.000191\n",
       "26        soil_type_16    0.000089\n",
       "44        soil_type_34    0.000085\n",
       "46        soil_type_36    0.000067\n",
       "33        soil_type_23    0.000053\n",
       "30        soil_type_20    0.000039\n",
       "14         soil_type_4    0.000031\n",
       "23        soil_type_13    0.000026\n",
       "25        soil_type_15    0.000006\n",
       "29        soil_type_19    0.000000\n",
       "28        soil_type_18    0.000000\n",
       "24        soil_type_14    0.000000\n",
       "37        soil_type_27    0.000000\n",
       "18         soil_type_8    0.000000\n",
       "43        soil_type_33    0.000000\n",
       "36        soil_type_26    0.000000\n",
       "45        soil_type_35    0.000000\n",
       "35        soil_type_25    0.000000\n",
       "17         soil_type_7    0.000000\n",
       "16         soil_type_6    0.000000\n",
       "34        soil_type_24    0.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = validated_model.bestModel.stages[-1]\n",
    "importances = [(k, v) for v, k in zip(best_model.featureImportances.toArray(), new_df.columns)] # 마지막은 wilderness_area\n",
    "pd.DataFrame(importances, columns=[\"feature\", \"importance\"]).sort_values(\"importance\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
