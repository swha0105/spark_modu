{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "other-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.jars = [\"/opt/jar/spark-xml_2.12-0.9.0.jar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "friendly-knitting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://0cc13c47e22b:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = spark://spark-master:7077, app id = app-20210227104443-0000)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@281bba07\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark= SparkSession\n",
    "    .builder()\n",
    "    .appName(\"graphX\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "flying-lotus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import com.databricks.spark.xml._\n",
       "rawXML: org.apache.spark.sql.DataFrame = [Article: struct<Abstract: struct<AbstractText: array<struct<_Label:string,_NlmCategory:string,_VALUE:string>>, CopyrightInformation: string>, ArticleDate: struct<Day: bigint, Month: bigint ... 2 more fields> ... 11 more fields>, ChemicalList: struct<Chemical: array<struct<NameOfSubstance:struct<_UI:string,_VALUE:string>,RegistryNumber:string>>> ... 22 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.databricks.spark.xml._\n",
    "\n",
    "val rawXML = spark.read.option(\"rowTag\", \"MedlineCitation\").xml(\"hdfs://namenode:8020/medline/*.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "virgin-geology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Article: struct (nullable = true)\n",
      " |    |-- Abstract: struct (nullable = true)\n",
      " |    |    |-- AbstractText: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- _Label: string (nullable = true)\n",
      " |    |    |    |    |-- _NlmCategory: string (nullable = true)\n",
      " |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- CopyrightInformation: string (nullable = true)\n",
      " |    |-- ArticleDate: struct (nullable = true)\n",
      " |    |    |-- Day: long (nullable = true)\n",
      " |    |    |-- Month: long (nullable = true)\n",
      " |    |    |-- Year: long (nullable = true)\n",
      " |    |    |-- _DateType: string (nullable = true)\n",
      " |    |-- ArticleTitle: string (nullable = true)\n",
      " |    |-- AuthorList: struct (nullable = true)\n",
      " |    |    |-- Author: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- AffiliationInfo: struct (nullable = true)\n",
      " |    |    |    |    |    |-- Affiliation: string (nullable = true)\n",
      " |    |    |    |    |-- CollectiveName: string (nullable = true)\n",
      " |    |    |    |    |-- ForeName: string (nullable = true)\n",
      " |    |    |    |    |-- Initials: string (nullable = true)\n",
      " |    |    |    |    |-- LastName: string (nullable = true)\n",
      " |    |    |    |    |-- Suffix: string (nullable = true)\n",
      " |    |    |    |    |-- _ValidYN: string (nullable = true)\n",
      " |    |    |-- _CompleteYN: string (nullable = true)\n",
      " |    |-- DataBankList: struct (nullable = true)\n",
      " |    |    |-- DataBank: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- AccessionNumberList: struct (nullable = true)\n",
      " |    |    |    |    |    |-- AccessionNumber: array (nullable = true)\n",
      " |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |    |-- DataBankName: string (nullable = true)\n",
      " |    |    |-- _CompleteYN: string (nullable = true)\n",
      " |    |-- ELocationID: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- _EIdType: string (nullable = true)\n",
      " |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |-- _ValidYN: string (nullable = true)\n",
      " |    |-- GrantList: struct (nullable = true)\n",
      " |    |    |-- Grant: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- Acronym: string (nullable = true)\n",
      " |    |    |    |    |-- Agency: string (nullable = true)\n",
      " |    |    |    |    |-- Country: string (nullable = true)\n",
      " |    |    |    |    |-- GrantID: string (nullable = true)\n",
      " |    |    |-- _CompleteYN: string (nullable = true)\n",
      " |    |-- Journal: struct (nullable = true)\n",
      " |    |    |-- ISOAbbreviation: string (nullable = true)\n",
      " |    |    |-- ISSN: struct (nullable = true)\n",
      " |    |    |    |-- _IssnType: string (nullable = true)\n",
      " |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- JournalIssue: struct (nullable = true)\n",
      " |    |    |    |-- Issue: string (nullable = true)\n",
      " |    |    |    |-- PubDate: struct (nullable = true)\n",
      " |    |    |    |    |-- Day: long (nullable = true)\n",
      " |    |    |    |    |-- MedlineDate: string (nullable = true)\n",
      " |    |    |    |    |-- Month: string (nullable = true)\n",
      " |    |    |    |    |-- Season: string (nullable = true)\n",
      " |    |    |    |    |-- Year: long (nullable = true)\n",
      " |    |    |    |-- Volume: string (nullable = true)\n",
      " |    |    |    |-- _CitedMedium: string (nullable = true)\n",
      " |    |    |-- Title: string (nullable = true)\n",
      " |    |-- Language: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |    |-- Pagination: struct (nullable = true)\n",
      " |    |    |-- MedlinePgn: string (nullable = true)\n",
      " |    |-- PublicationTypeList: struct (nullable = true)\n",
      " |    |    |-- PublicationType: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- _UI: string (nullable = true)\n",
      " |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |-- VernacularTitle: string (nullable = true)\n",
      " |    |-- _PubModel: string (nullable = true)\n",
      " |-- ChemicalList: struct (nullable = true)\n",
      " |    |-- Chemical: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- NameOfSubstance: struct (nullable = true)\n",
      " |    |    |    |    |-- _UI: string (nullable = true)\n",
      " |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |-- RegistryNumber: string (nullable = true)\n",
      " |-- CitationSubset: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- CommentsCorrectionsList: struct (nullable = true)\n",
      " |    |-- CommentsCorrections: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- Note: string (nullable = true)\n",
      " |    |    |    |-- PMID: struct (nullable = true)\n",
      " |    |    |    |    |-- _VALUE: long (nullable = true)\n",
      " |    |    |    |    |-- _Version: long (nullable = true)\n",
      " |    |    |    |-- RefSource: string (nullable = true)\n",
      " |    |    |    |-- _RefType: string (nullable = true)\n",
      " |-- DateCompleted: struct (nullable = true)\n",
      " |    |-- Day: long (nullable = true)\n",
      " |    |-- Month: long (nullable = true)\n",
      " |    |-- Year: long (nullable = true)\n",
      " |-- DateCreated: struct (nullable = true)\n",
      " |    |-- Day: long (nullable = true)\n",
      " |    |-- Month: long (nullable = true)\n",
      " |    |-- Year: long (nullable = true)\n",
      " |-- DateRevised: struct (nullable = true)\n",
      " |    |-- Day: long (nullable = true)\n",
      " |    |-- Month: long (nullable = true)\n",
      " |    |-- Year: long (nullable = true)\n",
      " |-- GeneSymbolList: struct (nullable = true)\n",
      " |    |-- GeneSymbol: array (nullable = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- GeneralNote: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _Owner: string (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |-- InvestigatorList: struct (nullable = true)\n",
      " |    |-- Investigator: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- AffiliationInfo: struct (nullable = true)\n",
      " |    |    |    |    |-- Affiliation: string (nullable = true)\n",
      " |    |    |    |-- ForeName: string (nullable = true)\n",
      " |    |    |    |-- Initials: string (nullable = true)\n",
      " |    |    |    |-- LastName: string (nullable = true)\n",
      " |    |    |    |-- Suffix: string (nullable = true)\n",
      " |    |    |    |-- _ValidYN: string (nullable = true)\n",
      " |-- KeywordList: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- Keyword: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- _MajorTopicYN: string (nullable = true)\n",
      " |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- _Owner: string (nullable = true)\n",
      " |-- MedlineJournalInfo: struct (nullable = true)\n",
      " |    |-- Country: string (nullable = true)\n",
      " |    |-- ISSNLinking: string (nullable = true)\n",
      " |    |-- MedlineTA: string (nullable = true)\n",
      " |    |-- NlmUniqueID: string (nullable = true)\n",
      " |-- MeshHeadingList: struct (nullable = true)\n",
      " |    |-- MeshHeading: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- DescriptorName: struct (nullable = true)\n",
      " |    |    |    |    |-- _MajorTopicYN: string (nullable = true)\n",
      " |    |    |    |    |-- _Type: string (nullable = true)\n",
      " |    |    |    |    |-- _UI: string (nullable = true)\n",
      " |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |    |-- QualifierName: array (nullable = true)\n",
      " |    |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |    |-- _MajorTopicYN: string (nullable = true)\n",
      " |    |    |    |    |    |-- _UI: string (nullable = true)\n",
      " |    |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |-- NumberOfReferences: long (nullable = true)\n",
      " |-- OtherAbstract: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- AbstractText: string (nullable = true)\n",
      " |    |    |-- CopyrightInformation: string (nullable = true)\n",
      " |    |    |-- _Language: string (nullable = true)\n",
      " |    |    |-- _Type: string (nullable = true)\n",
      " |-- OtherID: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _Source: string (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |-- PMID: struct (nullable = true)\n",
      " |    |-- _VALUE: long (nullable = true)\n",
      " |    |-- _Version: long (nullable = true)\n",
      " |-- PersonalNameSubjectList: struct (nullable = true)\n",
      " |    |-- PersonalNameSubject: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- ForeName: string (nullable = true)\n",
      " |    |    |    |-- Initials: string (nullable = true)\n",
      " |    |    |    |-- LastName: string (nullable = true)\n",
      " |    |    |    |-- Suffix: string (nullable = true)\n",
      " |-- SpaceFlightMission: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- SupplMeshList: struct (nullable = true)\n",
      " |    |-- SupplMeshName: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- _Type: string (nullable = true)\n",
      " |    |    |    |-- _UI: string (nullable = true)\n",
      " |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |-- _Owner: string (nullable = true)\n",
      " |-- _Status: string (nullable = true)\n",
      " |-- _VersionDate: string (nullable = true)\n",
      " |-- _VersionID: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawXML.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "banner-stamp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- MeshHeading: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- DescriptorName: struct (nullable = true)\n",
      " |    |    |    |-- _MajorTopicYN: string (nullable = true)\n",
      " |    |    |    |-- _Type: string (nullable = true)\n",
      " |    |    |    |-- _UI: string (nullable = true)\n",
      " |    |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |    |-- QualifierName: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- _MajorTopicYN: string (nullable = true)\n",
      " |    |    |    |    |-- _UI: string (nullable = true)\n",
      " |    |    |    |    |-- _VALUE: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "meshHeadlingList: org.apache.spark.sql.DataFrame = [MeshHeading: array<struct<DescriptorName:struct<_MajorTopicYN:string,_Type:string,_UI:string,_VALUE:string>,QualifierName:array<struct<_MajorTopicYN:string,_UI:string,_VALUE:string>>>>]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "val meshHeadlingList = rawXML.select(\"MeshHeadingList.MeshHeading\")\n",
    "meshHeadlingList.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "demanding-conducting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- DescriptorName: struct (nullable = true)\n",
      " |    |    |-- _MajorTopicYN: string (nullable = true)\n",
      " |    |    |-- _Type: string (nullable = true)\n",
      " |    |    |-- _UI: string (nullable = true)\n",
      " |    |    |-- _VALUE: string (nullable = true)\n",
      " |    |-- QualifierName: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- _MajorTopicYN: string (nullable = true)\n",
      " |    |    |    |-- _UI: string (nullable = true)\n",
      " |    |    |    |-- _VALUE: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MeshHeadlingElems: org.apache.spark.sql.DataFrame = [data: struct<DescriptorName: struct<_MajorTopicYN: string, _Type: string ... 2 more fields>, QualifierName: array<struct<_MajorTopicYN:string,_UI:string,_VALUE:string>>>]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val MeshHeadlingElems = meshHeadlingList.withColumn(\"data\", explode($\"MeshHeading\")).select(\"data\")\n",
    "MeshHeadlingElems.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adverse-baking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DescriptorName: struct (nullable = true)\n",
      " |    |-- _MajorTopicYN: string (nullable = true)\n",
      " |    |-- _Type: string (nullable = true)\n",
      " |    |-- _UI: string (nullable = true)\n",
      " |    |-- _VALUE: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "descriptorName: org.apache.spark.sql.DataFrame = [DescriptorName: struct<_MajorTopicYN: string, _Type: string ... 2 more fields>]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val descriptorName = MeshHeadlingElems.select(MeshHeadlingElems.col(\"data.DescriptorName\"))\n",
    "descriptorName.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "worthy-longitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|_MajorTopicYN|               topic|\n",
      "+-------------+--------------------+\n",
      "|            N|            Behavior|\n",
      "|            N|Congenital Abnorm...|\n",
      "|            N|    Disabled Persons|\n",
      "|            N|             Disease|\n",
      "|            Y|Intellectual Disa...|\n",
      "|            N|        Intelligence|\n",
      "|            Y|Maternal-Fetal Ex...|\n",
      "|            N|         Personality|\n",
      "|            N|           Pregnancy|\n",
      "|            Y|Pregnancy Complic...|\n",
      "|            N|          Psychology|\n",
      "|            N|        Reproduction|\n",
      "|            N|            Research|\n",
      "|            N|       Contraception|\n",
      "|            N|Contraceptive Agents|\n",
      "|            N|Family Planning S...|\n",
      "|            Y|           Pessaries|\n",
      "|            N|        Reproduction|\n",
      "|            Y|     Sperm Transport|\n",
      "|            Y|Spermatocidal Agents|\n",
      "+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "parsedDF: org.apache.spark.sql.DataFrame = [_MajorTopicYN: string, topic: string]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val parsedDF = descriptorName.select(descriptorName.col(\"DescriptorName._MajorTopicYN\"),\n",
    "                                    descriptorName.col(\"DescriptorName._VALUE\").as(\"topic\"))\n",
    "parsedDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "infrared-removal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|_MajorTopicYN|               topic|\n",
      "+-------------+--------------------+\n",
      "|            Y|Intellectual Disa...|\n",
      "|            Y|Maternal-Fetal Ex...|\n",
      "|            Y|Pregnancy Complic...|\n",
      "|            Y|           Pessaries|\n",
      "|            Y|     Sperm Transport|\n",
      "|            Y|Spermatocidal Agents|\n",
      "|            Y|Vaginal Creams,Fo...|\n",
      "|            Y|       Amniocentesis|\n",
      "|            Y|            Research|\n",
      "|            Y|    Cesarean Section|\n",
      "|            Y|     General Surgery|\n",
      "|            Y|        Hysterectomy|\n",
      "|            Y|Retrospective Stu...|\n",
      "|            Y|Sterilization,Rep...|\n",
      "|            Y|  Animals,Laboratory|\n",
      "|            Y|            Eugenics|\n",
      "|            Y|           Aftercare|\n",
      "|            Y|          Anesthesia|\n",
      "|            Y|    Cesarean Section|\n",
      "|            Y|       Contraception|\n",
      "+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "majorTopic: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_MajorTopicYN: string, topic: string]\n",
       "majorTopic: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [_MajorTopicYN: string, topic: string]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var majorTopic = parsedDF.filter(col(\"_MajorTopicYN\") === \"Y\")\n",
    "majorTopic = majorTopic.withColumn(\"topic\", regexp_replace(majorTopic(\"topic\"), \", \", \",\"))\n",
    "majorTopic.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-atmosphere",
   "metadata": {},
   "source": [
    "# MeSH 주요 주제와 주제들의 동시발생 분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beautiful-locking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               topic|count|\n",
      "+--------------------+-----+\n",
      "|            Research| 1649|\n",
      "|             Disease| 1349|\n",
      "|           Neoplasms| 1123|\n",
      "|        Tuberculosis| 1066|\n",
      "|       Public Policy|  816|\n",
      "|       Jurisprudence|  796|\n",
      "|          Demography|  763|\n",
      "| Population Dynamics|  753|\n",
      "|           Economics|  690|\n",
      "|            Medicine|  682|\n",
      "|Socioeconomic Fac...|  655|\n",
      "|               Blood|  631|\n",
      "|            Politics|  631|\n",
      "|Emigration and Im...|  601|\n",
      "|       Social Change|  577|\n",
      "|          Physicians|  560|\n",
      "|            Mutation|  542|\n",
      "|    Abortion,Induced|  503|\n",
      "|          Anesthesia|  483|\n",
      "|       Public Health|  479|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topicDist: org.apache.spark.sql.DataFrame = [topic: string, count: bigint]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topicDist = majorTopic.groupBy(\"topic\").count()\n",
    "topicDist.orderBy(desc(\"count\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beginning-inflation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topics: org.apache.spark.rdd.RDD[List[String]] = MapPartitionsRDD[41] at map at <console>:35\n",
       "onlyTopics: org.apache.spark.sql.DataFrame = [topic: string]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topics = majorTopic.select(\"topic\").rdd.map(el => el.getString(0).split(\",\").toList)\n",
    "val onlyTopics =  topics.flatMap(mesh => mesh).toDF(\"topic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-tours",
   "metadata": {},
   "source": [
    "## 문자열들의 목록에서 문자열 2개로 구성된 모든 부분 집합 만들기\n",
    "원소가 다르게 정렬되어 있는 리스트를 다른 리스트로 보기 때문에    \n",
    "미리 정렬을 해줘야한다. 그 뒤 scala의 서브 리스트를 만드는 combinations 메소드를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "optimum-concord",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|               pairs|cnt|\n",
      "+--------------------+---+\n",
      "|[Closed, Ecologic...| 31|\n",
      "|[Hypertension, Pu...| 18|\n",
      "|[Halogenated, Hyd...|  2|\n",
      "|[Ductus Arteriosu...| 11|\n",
      "|[Hypotension, Ort...|  5|\n",
      "|    [Biopsy, Needle]| 49|\n",
      "|[Intraocular, Len...| 46|\n",
      "|  [Artificial, Skin]| 12|\n",
      "|[Booksellers', Ca...|  1|\n",
      "|[Epistasis, Genetic]|  3|\n",
      "|[Denture, Resin-B...|  1|\n",
      "|[Comminuted, Frac...|  2|\n",
      "|  [Medical, Schools]| 84|\n",
      "|[Carcinoma, Renal...|  7|\n",
      "|[Epilepsies, Myoc...|  6|\n",
      "|[Congenital, Hip ...| 10|\n",
      "|[Gingivitis, Necr...|  8|\n",
      "|[Chemistry, Clini...|  8|\n",
      "|[Informal, Social...| 46|\n",
      "|     [Genes, Lethal]| 15|\n",
      "+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topicPairs: org.apache.spark.sql.DataFrame = [pairs: array<string>]\n",
       "cooccurs: org.apache.spark.sql.DataFrame = [pairs: array<string>, cnt: bigint]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topicPairs = topics.flatMap(t => {t.sorted.combinations(2)}).toDF(\"pairs\") \n",
    "topicPairs.createOrReplaceTempView(\"topic_pairs\")\n",
    "val cooccurs = spark.sql(\"\"\"\n",
    "    SELECT pairs, COUNT(*) cnt\n",
    "    FROM topic_pairs\n",
    "    GROUP BY pairs\"\"\")\n",
    "cooccurs.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "coral-plane",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WrappedArray(Abortion, Induced),503]\n",
      "[WrappedArray(Biological, Models),471]\n",
      "[WrappedArray(Education, Medical),467]\n",
      "[WrappedArray(Chromosomes, Human),404]\n",
      "[WrappedArray(Infant, Newborn),399]\n",
      "[WrappedArray(Models, Theoretical),387]\n",
      "[WrappedArray(Formal, Social Control),348]\n",
      "[WrappedArray(Attitudes, Health Knowledge),322]\n",
      "[WrappedArray(Attitudes, Practice),322]\n",
      "[WrappedArray(Health Knowledge, Practice),322]\n",
      "[WrappedArray(Antibiotics, Antitubercular),314]\n",
      "[WrappedArray(Tomography, X-Ray Computed),312]\n",
      "[WrappedArray(Operative, Surgical Procedures),305]\n",
      "[WrappedArray(Education, Nursing),260]\n",
      "[WrappedArray(Ethics, Medical),257]\n",
      "[WrappedArray(Diseases, Infant),256]\n",
      "[WrappedArray(Genetic, Transcription),254]\n",
      "[WrappedArray(Diagnosis, Differential),248]\n",
      "[WrappedArray(Medicinal, Plants),237]\n",
      "[WrappedArray(Bone, Fractures),229]\n"
     ]
    }
   ],
   "source": [
    "cooccurs.createOrReplaceTempView(\"cooccurs\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT pairs, cnt\n",
    "    FROM cooccurs\n",
    "    ORDER BY cnt DESC\n",
    "    LIMIT 20\"\"\").collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-fraud",
   "metadata": {},
   "source": [
    "## GraphX로 동시발생 네트워크 구성하기\n",
    "GraphX는 두 개의 특화된 RDD를 사용해서 그래프를 생성한다\n",
    "### VertextRDD[VD]는 RDD[(VertexId, VD)]의 구현\n",
    "    64 bit Long의 Key와 Value\n",
    "### EdgeRDD[ED]는 RDD[(VertexID, ED)]의 구현\n",
    "    srcVertexId와 dstVertexId와 Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-characteristic",
   "metadata": {},
   "source": [
    "## MD5 해시 알고리즘을 사용해서 Vertex에 64bit Key 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "demographic-marker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.nio.charset.StandardCharsets\n",
       "import java.security.MessageDigest\n",
       "hashID: (str: String)Long\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.nio.charset.StandardCharsets\n",
    "import java.security.MessageDigest\n",
    "\n",
    "def hashID(str: String): Long = {\n",
    "    val bytes = MessageDigest.getInstance(\"MD5\").digest(str.getBytes(StandardCharsets.UTF_8))\n",
    "    (bytes(0) & 0xFFL) |\n",
    "    ((bytes(1) & 0xFFL) << 8)  |\n",
    "    ((bytes(2) & 0xFFL) << 16) |\n",
    "    ((bytes(3) & 0xFFL) << 24) | \n",
    "    ((bytes(4) & 0xFFL) << 32) |\n",
    "    ((bytes(5) & 0xFFL) << 40) |\n",
    "    ((bytes(6) & 0xFFL) << 48) |\n",
    "    ((bytes(7) & 0xFFL) << 56)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-input",
   "metadata": {},
   "source": [
    "## VertexRDD - 64 bit Long의 Key와 Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dated-helping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+\n",
      "|hash                |topic                  |\n",
      "+--------------------+-----------------------+\n",
      "|5187539882274411027 |Intellectual Disability|\n",
      "|7326766905191375254 |Maternal-Fetal Exchange|\n",
      "|-8335899805523560725|Pregnancy Complications|\n",
      "|6668370814094416392 |Pessaries              |\n",
      "|2165268630633201617 |Sperm Transport        |\n",
      "|7410582300145776325 |Spermatocidal Agents   |\n",
      "|-2715598449643586692|Vaginal Creams         |\n",
      "|-5490501322948777906|Foams                  |\n",
      "|6797329277563646197 |and Jellies            |\n",
      "|-8710082263013033965|Amniocentesis          |\n",
      "|5873630755945588720 |Research               |\n",
      "|-6195495891807228194|Cesarean Section       |\n",
      "|-4584508663902919400|General Surgery        |\n",
      "|-6313877427211760081|Hysterectomy           |\n",
      "|3931245851669231450 |Retrospective Studies  |\n",
      "|7711775517543139760 |Sterilization          |\n",
      "|-5551980622857818596|Reproductive           |\n",
      "|6222546531752250163 |Animals                |\n",
      "|2732966919418709222 |Laboratory             |\n",
      "|-3989331720342451625|Eugenics               |\n",
      "+--------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\n",
       "vertices: org.apache.spark.sql.DataFrame = [hash: bigint, topic: string]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val vertices = onlyTopics.map{ case Row(topic: String) => (hashID(topic), topic) }.toDF(\"hash\", \"topic\")\n",
    "vertices.show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-employer",
   "metadata": {},
   "source": [
    "## EdgeRDD - srcVertexId와 dstVertexId와 Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "lucky-rwanda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.graphx._\n",
       "edges: org.apache.spark.sql.Dataset[org.apache.spark.graphx.Edge[Long]] = [srcId: bigint, dstId: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.graphx._\n",
    "\n",
    "val edges = cooccurs.map{ case Row(pairs: Seq[_], cnt: Long) =>\n",
    "    val ids = pairs.map(_.toString).map(hashID).sorted\n",
    "    Edge(ids(0), ids(1), cnt)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "southeast-native",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vertexRDD: org.apache.spark.rdd.RDD[(Long, String)] = MapPartitionsRDD[65] at map at <console>:43\n",
       "topicGraph: org.apache.spark.graphx.Graph[String,Long] = org.apache.spark.graphx.impl.GraphImpl@1d575146\n",
       "res10: org.apache.spark.graphx.Graph[String,Long] = org.apache.spark.graphx.impl.GraphImpl@1d575146\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val vertexRDD = vertices.rdd.map{\n",
    "    case Row(hash: Long, topic: String) => (hash, topic)\n",
    "}\n",
    "val topicGraph = Graph(vertexRDD, edges.rdd)\n",
    "topicGraph.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-tobacco",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/img/property_graph.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-sponsorship",
   "metadata": {},
   "source": [
    "## 모든 Vertex에서 다른 모든 Vertex로 이어지는 Path를 가지는 Connected Subgraph를 생성\n",
    "https://drek4537l1klr.cloudfront.net/bonaci/Figures/09fig03.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "worthy-onion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "connectedComponentGraph: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,Long] = org.apache.spark.graphx.impl.GraphImpl@f30442b\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val connectedComponentGraph = topicGraph.connectedComponents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "herbal-priest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|vid                 |cid                 |\n",
      "+--------------------+--------------------+\n",
      "|-7316262928265344779|-7316262928265344779|\n",
      "|6081464828435935967 |6081464828435935967 |\n",
      "|-3298299725015287104|-3298299725015287104|\n",
      "|1790883385483333827 |1790883385483333827 |\n",
      "|6371846346169607292 |6371846346169607292 |\n",
      "|9083303858128127359 |-9215470674759766104|\n",
      "|-2892570574836872675|-2892570574836872675|\n",
      "|4144959121306352236 |4144959121306352236 |\n",
      "|-2256043373786658093|-6731727939376452220|\n",
      "|7171990309415822392 |7171990309415822392 |\n",
      "|-7567449790538207579|-7567449790538207579|\n",
      "|-6856105093952105058|-6856105093952105058|\n",
      "|2277126264016938007 |-9146446762266834735|\n",
      "|-4510394959788367402|-4510394959788367402|\n",
      "|2026738476704047088 |2026738476704047088 |\n",
      "|-5074639714252992290|-5074639714252992290|\n",
      "|-3670369270656251168|-3670369270656251168|\n",
      "|3684435136196876397 |3684435136196876397 |\n",
      "|-265354314421783823 |-9215470674759766104|\n",
      "|8355045401803691586 |8355045401803691586 |\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "componentDF: org.apache.spark.sql.DataFrame = [vid: bigint, cid: bigint]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val componentDF = connectedComponentGraph.vertices.toDF(\"vid\", \"cid\")\n",
    "componentDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "green-vulnerability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "componentCounts: org.apache.spark.sql.DataFrame = [cid: bigint, count: bigint]\n",
       "res12: Long = 12185\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val componentCounts = componentDF.groupBy(\"cid\").count()\n",
    "componentCounts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-newcastle",
   "metadata": {},
   "source": [
    "## 각 연결 성분의 크기를 구해본다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "focused-provision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 cid|count|\n",
      "+--------------------+-----+\n",
      "|-9215470674759766104| 1229|\n",
      "|-8958016315901741476|   43|\n",
      "|-8534530815268459613|   31|\n",
      "|-8525945984722225322|   27|\n",
      "|-8433229734442232152|   23|\n",
      "|-5431966423110682938|   13|\n",
      "|-6457402890108996741|   13|\n",
      "|-8778799276582343892|   12|\n",
      "|-8952683923920091838|   12|\n",
      "|-2504097438637703249|   11|\n",
      "|-7654163459406679088|    9|\n",
      "|-7101834924453003464|    9|\n",
      "|-8900385585274452121|    8|\n",
      "|-9117649955724002678|    8|\n",
      "|-7694471437534469153|    8|\n",
      "|-2234424701633221593|    7|\n",
      "|-2442449324161724517|    7|\n",
      "|-7763756269967324984|    7|\n",
      "|-4801324290586934533|    7|\n",
      "|-8077949023993713521|    7|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "componentCounts.orderBy(desc(\"count\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-kinase",
   "metadata": {},
   "source": [
    "## cid count가 상위 7번째(13) 따리를 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "extensive-straight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 vid|                 cid|\n",
      "+--------------------+--------------------+\n",
      "| 8084519340451788661|-5431966423110682938|\n",
      "| 7095041754108558476|-5431966423110682938|\n",
      "| 7839629013181357020|-5431966423110682938|\n",
      "|-5120990266664752595|-5431966423110682938|\n",
      "|-1228431817674920367|-5431966423110682938|\n",
      "|-5431966423110682938|-5431966423110682938|\n",
      "| 3312266615297311284|-5431966423110682938|\n",
      "| 8179512113853959277|-5431966423110682938|\n",
      "|-4064173520721180199|-5431966423110682938|\n",
      "| 3497129956290009190|-5431966423110682938|\n",
      "| 4664620325710429792|-5431966423110682938|\n",
      "| 6775005617735922441|-5431966423110682938|\n",
      "|   28792762178501292|-5431966423110682938|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testGraphConnect: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [vid: bigint, cid: bigint]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testGraphConnect = componentDF.filter(col(\"cid\") === \"-5431966423110682938\")\n",
    "testGraphConnect.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-symphony",
   "metadata": {},
   "source": [
    "## Topic 을 확인하기 위해 Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "broad-nation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                 vid|                 cid|                hash|               topic|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "| 8084519340451788661|-5431966423110682938| 8084519340451788661|         Atherogenic|\n",
      "| 7839629013181357020|-5431966423110682938| 7839629013181357020|Carbohydrate-Rest...|\n",
      "|   28792762178501292|-5431966423110682938|   28792762178501292|          Cariogenic|\n",
      "|-4064173520721180199|-5431966423110682938|-4064173520721180199|            Diabetic|\n",
      "| 3497129956290009190|-5431966423110682938| 3497129956290009190|                Diet|\n",
      "| 4664620325710429792|-5431966423110682938| 4664620325710429792|      Fat-Restricted|\n",
      "| 3312266615297311284|-5431966423110682938| 3312266615297311284|         Gluten-Free|\n",
      "| 8179512113853959277|-5431966423110682938| 8179512113853959277|            High-Fat|\n",
      "| 7095041754108558476|-5431966423110682938| 7095041754108558476|       Mediterranean|\n",
      "|-5431966423110682938|-5431966423110682938|-5431966423110682938|  Protein-Restricted|\n",
      "|-5120990266664752595|-5431966423110682938|-5120990266664752595|            Reducing|\n",
      "|-1228431817674920367|-5431966423110682938|-1228431817674920367|   Sodium-Restricted|\n",
      "| 6775005617735922441|-5431966423110682938| 6775005617735922441|          Vegetarian|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "joinExp: org.apache.spark.sql.Column = (hash = vid)\n",
       "joinWithVertexName: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [vid: bigint, cid: bigint ... 2 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val joinExp = vertices.col(\"hash\") === testGraphConnect.col(\"vid\")\n",
    "val joinWithVertexName = testGraphConnect.join(vertices, joinExp).distinct()\n",
    "\n",
    "joinWithVertexName.orderBy(col(\"topic\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-fleet",
   "metadata": {},
   "source": [
    "## Diet와 유사한 주제 중 Edge에 포함되지 않은 주제는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "frank-traveler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               topic|count|\n",
      "+--------------------+-----+\n",
      "|    Dietary Services|    2|\n",
      "|Diet,Protein-Rest...|    6|\n",
      "|        Diet Therapy|   40|\n",
      "|           Diet Fads|    5|\n",
      "|     Calcium,Dietary|   60|\n",
      "|       Diet,High-Fat|    4|\n",
      "|     Diet,Vegetarian|    6|\n",
      "|Diet,Carbohydrate...|    1|\n",
      "| Cholesterol,Dietary|    6|\n",
      "|      Sodium,Dietary|   40|\n",
      "|       Dietary Fiber|   13|\n",
      "|     Dietary Sucrose|    1|\n",
      "|       Diethylamines|    1|\n",
      "|           Dietetics|   42|\n",
      "|    Diet,Gluten-Free|    2|\n",
      "|        Diet Surveys|   13|\n",
      "|Recommended Dieta...|    2|\n",
      "|    Dietary Proteins|   46|\n",
      "|  Diethylnitrosamine|    4|\n",
      "|Dietary Carbohydr...|   28|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topicDist.filter($\"topic\".contains(\"Diet\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "valued-maria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|     topic|count|\n",
      "+----------+-----+\n",
      "|Demography|  763|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topicDist.filter($\"topic\".contains(\"Demography\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-guide",
   "metadata": {},
   "source": [
    "## 차수( 각 Vertex에 연결된 Edge의 수)의 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ideal-edward",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "degrees: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[373] at RDD at VertexRDD.scala:57\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val degrees: VertexRDD[Int] = topicGraph.degrees.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-journal",
   "metadata": {},
   "source": [
    "## degrees 메소드로 반환한 그래프 각 Vertex의 차수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "positive-armor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            vertexID|degree|\n",
      "+--------------------+------+\n",
      "|-5431966423110682938|     1|\n",
      "+--------------------+------+\n",
      "\n",
      "2352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "testVertexDegree: org.apache.spark.sql.DataFrame = [vertexID: bigint, degree: int]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testVertexDegree = degrees.toDF(\"vertexID\", \"degree\")\n",
    "\n",
    "testVertexDegree.where(col(\"vertexID\") === \"-5431966423110682938\").show\n",
    "println(testVertexDegree.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-image",
   "metadata": {},
   "source": [
    "## 차수 분포의 기본적인 요약 통계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fitted-expert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|            degree|\n",
      "+-------+------------------+\n",
      "|  count|              2352|\n",
      "|   mean|1.9838435374149659|\n",
      "| stddev| 3.238130572112704|\n",
      "|    min|                 1|\n",
      "|    max|                61|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testVertexDegree.describe(\"degree\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "operating-polish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topicGraphVerteciesCount: Long = 14174\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topicGraphVerteciesCount = topicGraph.vertices.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "emerging-cameroon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "singleTopics: org.apache.spark.rdd.RDD[List[String]] = MapPartitionsRDD[390] at filter at <console>:41\n",
       "res20: Long = 245292\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val singleTopics = topics.filter(x => x.size == 1)\n",
    "singleTopics.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-running",
   "metadata": {},
   "source": [
    "## 전체 문서 중에 외톨이 주제의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "vocational-excerpt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "singleTopicsDistinct: org.apache.spark.sql.Dataset[String] = [value: string]\n",
       "res21: Long = 12389\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val singleTopicsDistinct = singleTopics.flatMap(topic => topic).distinct().toDS()\n",
    "singleTopicsDistinct.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-vertical",
   "metadata": {},
   "source": [
    "## topicPairs에 이미 포함되어 있는 것을 except 한 아무 것도 연결되어 있지 않은 Topic의 수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cellular-width",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "singleTopicInPairs: org.apache.spark.sql.Dataset[String] = [value: string]\n",
       "res22: Long = 11822\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val singleTopicInPairs = topicPairs.flatMap(_.getAs[Seq[String]](0))\n",
    "singleTopicsDistinct.except(singleTopicInPairs).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-gravity",
   "metadata": {},
   "source": [
    "## 그래프의 총 Vertex 수 = degrees RDD의 원소 수 + 외톨이 Topic 수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "effective-plain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res23: Boolean = true\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicGraphVerteciesCount == (testVertexDegree.select(\"degree\").count() + singleTopicsDistinct.except(singleTopicInPairs).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "colored-tournament",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namesAndDegrees: org.apache.spark.sql.DataFrame = [topic: string, degree: int]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val namesAndDegrees = degrees.innerJoin(topicGraph.vertices) {\n",
    "    (topicId, degree, name) => (name, degree.toInt)\n",
    "}.values.toDF(\"topic\", \"degree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-market",
   "metadata": {},
   "source": [
    "## Degree가 높은 Topic을 뽑아낸다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "automated-seminar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|       topic|degree|\n",
      "+------------+------+\n",
      "|       Genes|    61|\n",
      "|       Human|    57|\n",
      "|   Receptors|    40|\n",
      "|    Hospital|    38|\n",
      "| Chromosomes|    38|\n",
      "|      Dental|    35|\n",
      "|     Medical|    25|\n",
      "|    Antigens|    25|\n",
      "|     Genetic|    25|\n",
      "|   Hospitals|    24|\n",
      "|   Bacterial|    23|\n",
      "|Tuberculosis|    23|\n",
      "|       Viral|    23|\n",
      "|      Animal|    23|\n",
      "|   Carcinoma|    21|\n",
      "|     Nursing|    21|\n",
      "|  Artificial|    21|\n",
      "|         RNA|    19|\n",
      "|   Education|    18|\n",
      "|      Models|    18|\n",
      "+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "namesAndDegrees.orderBy(desc(\"degree\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-potential",
   "metadata": {},
   "source": [
    "## 관련성이 낮은 관계를 필터링하기 : Chi-Squared Test\n",
    "### 전체 문서의 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "higher-nursing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T: Long = 280464\n",
       "res25: org.apache.spark.broadcast.Broadcast[Long] = Broadcast(125)\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val T = majorTopic.count() \n",
    "sc.broadcast(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-introduction",
   "metadata": {},
   "source": [
    "### RDD로 만드는 해쉬 값과 Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hydraulic-whole",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topicDistRdd: org.apache.spark.rdd.RDD[(Long, Long)] = MapPartitionsRDD[482] at rdd at <console>:43\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topicDistRdd = topicDist.map{\n",
    "    case Row(topic: String, count: Long) => (hashID(topic), count)\n",
    "}.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "received-geneva",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topicDistGraph: org.apache.spark.graphx.Graph[Long,Long] = org.apache.spark.graphx.impl.GraphImpl@5562fa59\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topicDistGraph = Graph(topicDistRdd, topicGraph.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "visible-black",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chiSq: (YY: Long, YB: Long, YA: Long, T: Long)Double\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chiSq(YY: Long, YB: Long, YA: Long, T: Long): Double = {\n",
    "    val NB = T - YB // B가 나오지 않음\n",
    "    val NA = T - YA // A가 나오지 않음\n",
    "    val YN = YA - YY // A 나오고 B 나오지 않음\n",
    "    val NY = YB - YY // A 나오지 않고 B 나옴\n",
    "    val NN = T - NY - YN - YY // A 와 B 모두 나오지 않음\n",
    "    val inner = math.abs(YY * NN - YN * NY) - T / 2.0 // 카이제곱 통계량 1\n",
    "    T * math.pow(inner, 2) / (YA * NA * YB * NB) // 카이제곱 통계량 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-potter",
   "metadata": {},
   "source": [
    "## EdgeTriplet 자료 구조 ( srcVertexID, Attr , DstVertexID)\n",
    "https://spark.apache.org/docs/latest/api/scala/org/apache/spark/graphx/EdgeTriplet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "reflected-following",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----+--------------------+-------+\n",
      "|srcId|             srcAttr|attr|               dstId|dstAttr|\n",
      "+-----+--------------------+----+--------------------+-------+\n",
      "|    0|-9182438712389191341|   5| -389298284109391318|     54|\n",
      "|    0|-7872097114296777293|   2| 1202294856466702536|     16|\n",
      "|    0|-7149230680597744952|   2| 5589473318573193898|      0|\n",
      "|    0|-6680067852188841900|   1| 7818223270735667332|      0|\n",
      "|  183|-6279404004170606079|  18|-6078785589870891357|      0|\n",
      "|    0|-5774599100051916967|   3| 6136359576808376643|      0|\n",
      "|    0|-3918893881484148728|   1| 4545427657671254648|      0|\n",
      "|   15|-1512285294743677509|  46| 1023245654305693193|      0|\n",
      "|  114|-1191649091021856552|  49| 2207317118408894164|      0|\n",
      "|    0| 2125035285081874317|  12| 3011389593876829075|    180|\n",
      "|    0| 2832577980852704136|  31| 8607497299412972547|      0|\n",
      "|   20| 3409758096235920035|  11| 6323290579067087369|      0|\n",
      "|    0|-7957937824582824477|  15|-4129130605120804884|    200|\n",
      "|   22|-7880128184377850423|   1|-1121093282196584963|      0|\n",
      "|    0|-6909507947153669123|   1| -814792847199990802|      0|\n",
      "|    0|-6773254738010264547|   8| 6168625905479503748|     45|\n",
      "|    2|-6425544260275936222|   1|  -85933708055143339|      0|\n",
      "|    0|-5796656999191013673|   1|  -18067588879036029|     19|\n",
      "|    0|-5141384577434428633|  46|  588231690861585522|      0|\n",
      "|    8|-2857320593425826927|   8|-1824314586863108793|      0|\n",
      "+-----+--------------------+----+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topicDistGraphTriplet: org.apache.spark.sql.DataFrame = [srcId: bigint, srcAttr: bigint ... 3 more fields]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topicDistGraphTriplet = topicDistGraph.triplets.map(triplet => \n",
    "    (triplet.srcAttr, triplet.srcId, triplet.attr, triplet.dstId, triplet.dstAttr))\n",
    "    .toDF(\"srcId\", \"srcAttr\",  \"attr\", \"dstId\", \"dstAttr\")\n",
    "topicDistGraphTriplet.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-petersburg",
   "metadata": {},
   "source": [
    "## Triplet의 값을 사용해서 Chi-Square Test 결과를 Edge로 하는 새로운 그래프를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "democratic-combat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chiSquaredGraph: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@d461e68\n",
       "res27: org.apache.spark.util.StatCounter = (count: 2333, mean: NaN, stdev: NaN, max: Infinity, min: 0.355924)\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val chiSquaredGraph = topicDistGraph.mapTriplets(triplet => {\n",
    "    chiSq(triplet.attr, triplet.srcAttr, triplet.dstAttr, T)\n",
    "})\n",
    "chiSquaredGraph.edges.map(x => x.attr).stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-paraguay",
   "metadata": {},
   "source": [
    "## ChiSquared 결과가 극단적인 19.5 이상으로 상대적으로 의미 없는 Edge를 제거한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "initial-bearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interesting: org.apache.spark.graphx.Graph[Long,Double] = org.apache.spark.graphx.impl.GraphImpl@71d8ecfe\n",
       "res28: Long = 2326\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val interesting = chiSquaredGraph.subgraph(\n",
    "    triplet => triplet.attr > 19.5)\n",
    "interesting.edges.count  // 0 - 2333, 19.5 2326"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-herald",
   "metadata": {},
   "source": [
    "## Edge가 필터링된 그래프를 분석한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "centered-prospect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interestingComponentGraph: org.apache.spark.graphx.Graph[org.apache.spark.graphx.VertexId,Double] = org.apache.spark.graphx.impl.GraphImpl@38d14997\n",
       "icDF: org.apache.spark.sql.DataFrame = [vid: bigint, cid: bigint]\n",
       "icCountDF: org.apache.spark.sql.DataFrame = [cid: bigint, count: bigint]\n",
       "res29: Long = 14346\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val interestingComponentGraph = interesting.connectedComponents()\n",
    "val icDF = interestingComponentGraph.vertices.toDF(\"vid\", \"cid\")\n",
    "val icCountDF = icDF.groupBy(\"cid\").count()\n",
    "icCountDF.count() // Edge의 수 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-geography",
   "metadata": {},
   "source": [
    "## Edge의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "minus-mention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 cid|count|\n",
      "+--------------------+-----+\n",
      "|-9215470674759766104| 1213|\n",
      "|-8958016315901741476|   43|\n",
      "|-8534530815268459613|   31|\n",
      "|-8525945984722225322|   27|\n",
      "|-8433229734442232152|   23|\n",
      "|-9195187390356502787|   15|\n",
      "|-5431966423110682938|   13|\n",
      "|-6457402890108996741|   13|\n",
      "|-8952683923920091838|   12|\n",
      "|-8778799276582343892|   12|\n",
      "|-2504097438637703249|   11|\n",
      "|-7101834924453003464|    9|\n",
      "|-7654163459406679088|    9|\n",
      "|-8900385585274452121|    8|\n",
      "|-9117649955724002678|    8|\n",
      "|-7694471437534469153|    8|\n",
      "|-2234424701633221593|    7|\n",
      "|-7763756269967324984|    7|\n",
      "|-4801324290586934533|    7|\n",
      "|-8077949023993713521|    7|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "icCountDF.orderBy(desc(\"count\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-medicare",
   "metadata": {},
   "source": [
    "## 필터링 된 차수의 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "prescribed-shoulder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interestingDegrees: org.apache.spark.graphx.VertexRDD[Int] = VertexRDDImpl[757] at RDD at VertexRDD.scala:57\n",
       "res31: org.apache.spark.util.StatCounter = (count: 2351, mean: 1.978732, stdev: 3.228102, max: 61.000000, min: 1.000000)\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val interestingDegrees = interesting.degrees.cache()\n",
    "interestingDegrees.map(_._2).stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-drive",
   "metadata": {},
   "source": [
    "## 필터링 된 그래프에서의 Topic과 Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "structured-advice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|       topic|degree|\n",
      "+------------+------+\n",
      "|       Genes|    61|\n",
      "|       Human|    57|\n",
      "|   Receptors|    40|\n",
      "|    Hospital|    38|\n",
      "| Chromosomes|    38|\n",
      "|      Dental|    35|\n",
      "|     Genetic|    25|\n",
      "|    Antigens|    25|\n",
      "|     Medical|    25|\n",
      "|   Hospitals|    24|\n",
      "|      Animal|    23|\n",
      "|       Viral|    23|\n",
      "|   Bacterial|    23|\n",
      "|Tuberculosis|    22|\n",
      "|  Artificial|    21|\n",
      "|     Nursing|    21|\n",
      "|   Carcinoma|    20|\n",
      "|         RNA|    19|\n",
      "|   Education|    18|\n",
      "|      Models|    18|\n",
      "+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interestingDegrees.innerJoin(topicGraph.vertices) {\n",
    "    (topicId, degree, name) => (name, degree)\n",
    "}.values.toDF(\"topic\", \"degree\").orderBy(desc(\"degree\")).show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-confirmation",
   "metadata": {},
   "source": [
    "## 고급 그래프 속성 몇가지를 계산\n",
    "    Collective Dynamics of 'Small-world Networks'\n",
    "    완전 그래프 - 그래프에서 서로 다른 모든 Vertex가 반드시 연결되어 있는 그래프\n",
    "    \n",
    "    어떤 그래프가 완전 부분그래프(Clique)를 가지는 것을 찾는것이 NP-Complete Prob\n",
    "    간접적으로 Triangle Count( Vertex 세개로 이루어진 완전 그래프)를 이용한다.\n",
    "    Triangle Count를 사용하는 지역 군집 계수 ( Local Clustering Coefficient )\n",
    "    식 = 실제 존재하는 트라이 앵글 수  /  만들 수 있는 전체 트라이앵글 수\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-saying",
   "metadata": {},
   "source": [
    "## 각 Vertex의 트라이앵글 수를 가진 그래프를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cognitive-encyclopedia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "triCountGraph: org.apache.spark.graphx.Graph[Int,Double] = org.apache.spark.graphx.impl.GraphImpl@2ffad3b2\n",
       "res33: org.apache.spark.util.StatCounter = (count: 16333, mean: 0.029756, stdev: 0.464996, max: 34.000000, min: 0.000000)\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val triCountGraph = interesting.triangleCount()\n",
    "triCountGraph.vertices.map(x => x._2).stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-webster",
   "metadata": {},
   "source": [
    "## 한 Vertex의 전체 트라이앵글 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "roman-train",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxTrisGraph: org.apache.spark.graphx.VertexRDD[Double] = VertexRDDImpl[821] at RDD at VertexRDD.scala:57\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val maxTrisGraph = interestingDegrees.mapValues(d => d * (d-1) / 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-movie",
   "metadata": {},
   "source": [
    "## 두 그래프를 조인해서  지역 군집계수를 구한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "behind-oracle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clusterCoef: org.apache.spark.graphx.VertexRDD[Double] = VertexRDDImpl[823] at RDD at VertexRDD.scala:57\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val clusterCoef = triCountGraph.vertices.innerJoin(maxTrisGraph) { \n",
    "    (vertexId, triCount, maxTris) => {if (maxTris == 0) 0 else triCount / maxTris}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-dylan",
   "metadata": {},
   "source": [
    "## 네트워크의 평균 군집 계수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "protecting-triumph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res34: Double = 0.009933650080270988\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterCoef.map(_._2).sum() / interesting.vertices.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-surge",
   "metadata": {},
   "source": [
    "## Pregel을 사용해서 평균 경로 길이 계산하기\n",
    "    Vertex간 경로(Path)의 길이를 구하기\n",
    "~~~\n",
    "        for\n",
    "            => 각 꼭짓점과 그 꼭짓점 까지의 거리의 목록을 만든다.\n",
    "            => 아웃 노드들에 그들이 보유한 목록을 질의한다.\n",
    "            => 자신에게 없는 꼭짓점 까지의 거리를 추가 갱신한다.\n",
    "        end 더이상 추가할 수 없을 때 까지 반복한다.\n",
    "~~~\n",
    "    Pregel 은 Computation(계산)과 Communication(통신) 두 단계로 나누어 병렬 프로그래밍\n",
    "\n",
    "        Computation : 각 Vertex에서 내부 상태를 검사해 다른 Vertex에 'Message'를 보낼 것을 정한다.\n",
    "        Communication : 이전 통신 단계의 결과로 나온 메시지를 적당한 Vertex로 보낼 수 있도록 경로를 지정\n",
    "\n",
    "    Pregel API 를 사용할 때 구현해야할 함수\n",
    "        1. 각 Vertex의 상태를 추적하는 함수 ( 어떤 상태를 추적할 건지 )\n",
    "        2. 이웃하는 Vertex의 각 쌍을 평가한 후, 다음 단계에 보낼 함수를 정하는 함수\n",
    "        3. 받은 메시지 들을 통합해서 자신(Vertex)의 상태를 업데이트 함수\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-medicine",
   "metadata": {},
   "source": [
    "## 평균 경로 길이 문제 \n",
    "    각 Vertex의 상태 : 알려진 다른 ( VertexId, 거리 ) -> Map[VertexId, Int] ( 룩업 테이블)\n",
    "    각 Vertex에 전달할 메시지 : ( VertexId, 거리 ) -> Map[VertexId, Int] ( 룩업 테이블 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-court",
   "metadata": {},
   "source": [
    "## 도착한 메시지 정보를 Vertexd의 상태에 병합하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "industrial-shadow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mergeMaps: (m1: Map[org.apache.spark.graphx.VertexId,Int], m2: Map[org.apache.spark.graphx.VertexId,Int])Map[org.apache.spark.graphx.VertexId,Int]\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mergeMaps(m1: Map[VertexId, Int], m2: Map[VertexId, Int]) : Map[VertexId, Int] = {\n",
    "    def minThatExists(k: VertexId): Int = {\n",
    "        math.min(m1.getOrElse(k, Int.MaxValue), m2.getOrElse(k, Int.MaxValue))\n",
    "    }\n",
    "    \n",
    "    (m1.keySet ++ m2.keySet).map(k => (k, minThatExists(k))).toMap // 동시에 나타는 VertexId에 대해서 더 작은 값을 보관한다\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-crossing",
   "metadata": {},
   "source": [
    "## Vertex 업데이트 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "suited-ozone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update: (id: org.apache.spark.graphx.VertexId, state: Map[org.apache.spark.graphx.VertexId,Int], msg: Map[org.apache.spark.graphx.VertexId,Int])Map[org.apache.spark.graphx.VertexId,Int]\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update(id: VertexId, state: Map[VertexId, Int], msg: Map[VertexId, Int]) = {\n",
    "    mergeMaps(state, msg)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-tampa",
   "metadata": {},
   "source": [
    "## 이웃하는 Vertex에서 받은 정보를 보고 각 Vertex에 보낼 메시지 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "controlling-cologne",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "checkIncrement: (a: Map[org.apache.spark.graphx.VertexId,Int], b: Map[org.apache.spark.graphx.VertexId,Int], bid: org.apache.spark.graphx.VertexId)Iterator[(org.apache.spark.graphx.VertexId, scala.collection.immutable.Map[org.apache.spark.graphx.VertexId,Int])]\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def checkIncrement(a: Map[VertexId, Int], b: Map[VertexId, Int], bid: VertexId) = {\n",
    "    val aplus = a.map { case (v, d) => v -> (d+1) } // 거리 1 증가\n",
    "    if(b != mergeMaps(aplus, b)) {\n",
    "        Iterator((bid, aplus)) // 이웃 Vertex의 상태와 다르다면 이웃 Vertex에 결과를 보낼 메시지 생성\n",
    "    } else {\n",
    "        Iterator.empty\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-collapse",
   "metadata": {},
   "source": [
    "## src 와 dst 양 Vertex에서 메시지를 갱신 시 사용할 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "arabic-percentage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iterate: (e: org.apache.spark.graphx.EdgeTriplet[Map[org.apache.spark.graphx.VertexId,Int], _])Iterator[(org.apache.spark.graphx.VertexId, scala.collection.immutable.Map[org.apache.spark.graphx.VertexId,Int])]\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def iterate(e: EdgeTriplet[Map[VertexId, Int], _]) = {\n",
    "    checkIncrement(e.srcAttr, e.dstAttr, e.dstId) ++\n",
    "    checkIncrement(e.dstAttr, e.srcAttr, e.srcId)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-thomson",
   "metadata": {},
   "source": [
    "## Vertex를 Sampling 해서 새로운 Graph를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "nuclear-transparency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fraction: Double = 0.02\n",
       "replacement: Boolean = false\n",
       "sample: org.apache.spark.rdd.RDD[org.apache.spark.graphx.VertexId] = PartitionwiseSampledRDD[827] at sample at <console>:43\n",
       "ids: scala.collection.immutable.Set[Long] = Set(-2946899861498087798, -7913216201026662864, -76428294520429966, 2499204606670907436, -3285152267756629768, -9005957016891921496, 6037965845032188647, 5668309178822636501, 3993598833166579669, 1191901616057028428, 8064563839628739696, -925832195390866027, -8174457967231783561, -1602707214362909118, 5479593410478892393, -3604279977562363738, 3126407477913472028, -7596449335273103275, 912942725752082835, -6879831100030416881, -4815080670761796817, -6266048249759281991, 5813898916814956936, 7720856311546693293, 479674692011546890, 7964233329544335291, -37787311291635943...\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fraction = 0.02\n",
    "val replacement = false\n",
    "val sample = interesting.vertices.map(v => v._1).sample(replacement, fraction, 1729L)\n",
    "val ids = sample.collect().toSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "interstate-abuse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mapGraph: org.apache.spark.graphx.Graph[scala.collection.immutable.Map[org.apache.spark.graphx.VertexId,Int],Double] = org.apache.spark.graphx.impl.GraphImpl@17d6b899\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mapGraph = interesting.mapVertices((id, _) => {\n",
    "    if (ids.contains(id)) {\n",
    "        Map(id -> 0)\n",
    "    } else {\n",
    "        Map[VertexId, Int]()\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-stewart",
   "metadata": {},
   "source": [
    "## Vertex의 메시지를 선언하고 ( 비어있음 ) pregel 메소드를 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "grand-illinois",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "start: scala.collection.immutable.Map[org.apache.spark.graphx.VertexId,Int] = Map()\n",
       "res: org.apache.spark.graphx.Graph[scala.collection.immutable.Map[org.apache.spark.graphx.VertexId,Int],Double] = org.apache.spark.graphx.impl.GraphImpl@207d1065\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val start = Map[VertexId, Int]()\n",
    "val res = mapGraph.pregel(start)(update, iterate, mergeMaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-blend",
   "metadata": {},
   "source": [
    "## Path 길이를 계산한다. ( VertexId, VertexId, Int )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "brave-network",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paths: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.VertexId, Int)] = MapPartitionsRDD[1158] at distinct at <console>:41\n",
       "res36: paths.type = MapPartitionsRDD[1158] at distinct at <console>:41\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val paths = res.vertices.flatMap{ case(id, m) => // VertexId, ( VertexId, Int )\n",
    "    m.map { case(k, v) => // ( VertexId, Int )\n",
    "        if (id < k) {\n",
    "            (id, k, v)\n",
    "        } else {\n",
    "            (id, k, v)\n",
    "        }\n",
    "    }\n",
    "}.distinct()\n",
    "paths.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "saving-artwork",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|         SrcVertexId|         DstVertexId|PathLen|\n",
      "+--------------------+--------------------+-------+\n",
      "|  395415489013661543|-6064124848399142155|      8|\n",
      "|-1953899191912595410|-6064124848399142155|      6|\n",
      "|-7693150486571694714| 4148042476509963484|      9|\n",
      "|  311563963288822342|-6475986039297157456|      7|\n",
      "|-7683845844397460107|-5512369437406212956|      5|\n",
      "|    9929880474613702| 3080671367692401373|      8|\n",
      "| 6715451969093686253| -112645709515239560|      5|\n",
      "|-3858832936894511624|-6475986039297157456|      7|\n",
      "|-1295125037716930763|-1521890788461760039|      8|\n",
      "|-2089196482998081636| -112645709515239560|      7|\n",
      "| 6222546531752250163|-6266048249759281991|      7|\n",
      "|-2233884578870028995| 8834212968550452686|      6|\n",
      "| -697814771640862887| 1695642729486467421|      6|\n",
      "| -231263932096822161|-7398369319326415525|     11|\n",
      "| 7170544481691569991| 4637386053499667192|      8|\n",
      "| -673151094204633006|-2766443787435473830|      8|\n",
      "|-6729516762811890516| -925832195390866027|      8|\n",
      "| -505607410402862261| 6325013948685856600|      7|\n",
      "| 7451680736601402902| -112645709515239560|      8|\n",
      "|-7998339368461761178| 1695642729486467421|      5|\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pathDF: org.apache.spark.sql.DataFrame = [SrcVertexId: bigint, DstVertexId: bigint ... 1 more field]\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pathDF = paths.toDF(\"SrcVertexId\", \"DstVertexId\", \"PathLen\")\n",
    "pathDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "demonstrated-claim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|           PathLen|\n",
      "+-------+------------------+\n",
      "|  count|             26889|\n",
      "|   mean| 7.103164862955111|\n",
      "| stddev|2.4962618636291096|\n",
      "|    min|                 1|\n",
      "|    max|                18|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pathDF.filter(col(\"PathLen\") > 0).describe(\"PathLen\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "complimentary-roommate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|PathLen|count|\n",
      "+-------+-----+\n",
      "|     12|  787|\n",
      "|      1|   82|\n",
      "|     13|  362|\n",
      "|      6| 4261|\n",
      "|     16|   26|\n",
      "|      3| 1099|\n",
      "|      5| 3408|\n",
      "|     15|   81|\n",
      "|      9| 2708|\n",
      "|     17|    9|\n",
      "|      4| 2150|\n",
      "|      8| 4150|\n",
      "|      7| 4188|\n",
      "|     10| 1749|\n",
      "|     11| 1194|\n",
      "|     14|  140|\n",
      "|      2|  494|\n",
      "|      0|  321|\n",
      "|     18|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pathDF.groupBy(\"PathLen\").count().show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
