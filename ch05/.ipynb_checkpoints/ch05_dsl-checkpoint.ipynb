{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "biblical-navigator",
   "metadata": {},
   "source": [
    "## Ch5: Anomaly Detection in Network Traffic with K-mean Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-equipment",
   "metadata": {},
   "source": [
    "### Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "allied-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, StandardScaler, PCA\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "solid-moisture",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXECUTOR_MEMORY = \"2g\"\n",
    "EXECUTOR_CORES = 2\n",
    "EXECUTORE_INSTANCES = 3\n",
    "DRIVER_MEMORY = \"1g\"\n",
    "DRIVER_MAX_RESULT_SIZE = \"1g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "determined-bread",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.memory', '2g'),\n",
       " ('spark.executor.instances', '3'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.kryoserializer.buffer.max', '1024m'),\n",
       " ('spark.app.id', 'local-1612059470767'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'CH5'),\n",
       " ('spark.driver.maxResultSize', '1g'),\n",
       " ('spark.driver.memory', '1g'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.driver.host', 'ce648118105d'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '42146'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(f\"CH5\")\n",
    "    .config(\"spark.executor.memory\", EXECUTOR_MEMORY)\n",
    "    .config(\"spark.executor.cores\", EXECUTOR_CORES)\n",
    "    .config(\"spark.executor.instances\", EXECUTORE_INSTANCES)\n",
    "    .config(\"spark.driver.memory\", DRIVER_MEMORY)\n",
    "    .config(\"spark.driver.maxResultSize\", DRIVER_MAX_RESULT_SIZE)\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\n",
    "#     .config(\"spark.sql.warehouse.dir\", \"/user/bigdata/members/shyeon/advanced-spark/data\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-experiment",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "focal-gibraltar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------+----+---------+---------+----+--------------+------+---+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+-------+\n",
      "|duration|protocol_type|service|flag|src_bytes|dst_bytes|land|wrong_fragment|urgent|hot|num_failed_logins|logged_in|num_compromised|root_shell|su_attempted|num_root|num_file_creations|num_shells|num_access_files|num_outbound_cmds|is_host_login|is_guest_login|count|srv_count|serror_rate|srv_serror_rate|rerror_rate|srv_rerror_rate|same_srv_rate|diff_srv_rate|srv_diff_host_rate|dst_host_count|dst_host_srv_count|dst_host_same_srv_rate|dst_host_diff_srv_rate|dst_host_same_src_port_rate|dst_host_srv_diff_host_rate|dst_host_serror_rate|dst_host_srv_serror_rate|dst_host_rerror_rate|dst_host_srv_rerror_rate|  label|\n",
      "+--------+-------------+-------+----+---------+---------+----+--------------+------+---+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+-------+\n",
      "|       0|          tcp|   http|  SF|      215|    45076|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|          0.0|           0.0|  1.0|      1.0|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           0.0|               0.0|                   0.0|                   0.0|                        0.0|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0|normal.|\n",
      "|       0|          tcp|   http|  SF|      162|     4528|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|          0.0|           0.0|  2.0|      2.0|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           1.0|               1.0|                   1.0|                   0.0|                        1.0|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0|normal.|\n",
      "+--------+-------------+-------+----+---------+---------+----+--------------+------+---+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = [\n",
    "    T.StructField(\"duration\", T.IntegerType(), True),\n",
    "    T.StructField(\"protocol_type\", T.StringType(), True),\n",
    "    T.StructField(\"service\", T.StringType(), True),\n",
    "    T.StructField(\"flag\", T.StringType(), True), \n",
    "    T.StructField(\"src_bytes\", T.IntegerType(), True), \n",
    "    T.StructField(\"dst_bytes\", T.IntegerType(), True), \n",
    "    T.StructField(\"land\", T.IntegerType(), True), \n",
    "    T.StructField(\"wrong_fragment\", T.IntegerType(), True),\n",
    "    T.StructField(\"urgent\", T.IntegerType(), True),\n",
    "    T.StructField(\"hot\", T.IntegerType(), True),\n",
    "    T.StructField(\"num_failed_logins\", T.IntegerType(), True),\n",
    "    T.StructField(\"logged_in\", T.IntegerType(), True),\n",
    "    T.StructField(\"num_compromised\", T.IntegerType(), True),\n",
    "    T.StructField(\"root_shell\", T.IntegerType(), True),\n",
    "    T.StructField(\"su_attempted\", T.IntegerType(), True),\n",
    "    T.StructField(\"num_root\", T.IntegerType(), True),\n",
    "    T.StructField(\"num_file_creations\", T.IntegerType(), True),\n",
    "    T.StructField(\"num_shells\", T.IntegerType(), True),\n",
    "    T.StructField(\"num_access_files\", T.IntegerType(), True),\n",
    "    T.StructField(\"num_outbound_cmds\", T.IntegerType(), True),\n",
    "    T.StructField(\"is_host_login\", T.DoubleType(), True),\n",
    "    T.StructField(\"is_guest_login\", T.DoubleType(), True),\n",
    "    T.StructField(\"count\", T.DoubleType(), True),\n",
    "    T.StructField(\"srv_count\", T.DoubleType(), True),\n",
    "    T.StructField(\"serror_rate\", T.DoubleType(), True),\n",
    "    T.StructField(\"srv_serror_rate\", T.DoubleType(), True),\n",
    "    T.StructField(\"rerror_rate\", T.DoubleType(), True),\n",
    "    T.StructField(\"srv_rerror_rate\", T.DoubleType(), True),\n",
    "    T.StructField(\"same_srv_rate\", T.DoubleType(), True),\n",
    "    T.StructField(\"diff_srv_rate\", T.DoubleType(), True),\n",
    "    T.StructField(\"srv_diff_host_rate\", T.DoubleType(), True),\n",
    "    T.StructField(\"dst_host_count\", T.DoubleType(), True),\n",
    "    T.StructField(\"dst_host_srv_count\", T.DoubleType(), True),\n",
    "    T.StructField(\"dst_host_same_srv_rate\", T.DoubleType(), True),\n",
    "    T.StructField(\"dst_host_diff_srv_rate\", T.DoubleType(), True),\n",
    "    T.StructField(\"dst_host_same_src_port_rate\", T.DoubleType(), True),\n",
    "    T.StructField(\"dst_host_srv_diff_host_rate\", T.DoubleType(), True),\n",
    "    T.StructField(\"dst_host_serror_rate\", T.DoubleType(), True),\n",
    "    T.StructField(\"dst_host_srv_serror_rate\", T.DoubleType(), True),\n",
    "    T.StructField(\"dst_host_rerror_rate\", T.DoubleType(), True),\n",
    "    T.StructField(\"dst_host_srv_rerror_rate\", T.DoubleType(), True),\n",
    "    T.StructField(\"label\", T.StringType(), True)\n",
    "    ]\n",
    "\n",
    "schema = T.StructType(schema)\n",
    "\n",
    "df = (spark\n",
    "      .read.format(\"csv\")\n",
    "      .option(\"header\", False)\n",
    "      .option(\"sep\", \",\")\n",
    "      .schema(schema)\n",
    "#       .option(\"inferSchema\", True)\n",
    "      .load(\"./data/kddcup.data.corrected\"))\n",
    "\n",
    "# df.printSchema()\n",
    "df.show(2)\n",
    "# df.select(\"label\").show(5) # 모든 컬럼의 스키마가 반영되었는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exposed-holly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+--------------------+\n",
      "|           label|  count|             percent|\n",
      "+----------------+-------+--------------------+\n",
      "|          smurf.|2807886|  0.5732215070499105|\n",
      "|        neptune.|1072017|  0.2188490559528143|\n",
      "|         normal.| 972781| 0.19859032412623553|\n",
      "|          satan.|  15892|0.003244304145551...|\n",
      "|        ipsweep.|  12481| 0.00254795872392609|\n",
      "|      portsweep.|  10413|0.002125782725121...|\n",
      "|           nmap.|   2316|  4.7280445514084E-4|\n",
      "|           back.|   2203|4.497358439875952E-4|\n",
      "|    warezclient.|   1020|2.082299413832715E-4|\n",
      "|       teardrop.|    979|1.998599143276694...|\n",
      "|            pod.|    264|5.389480835802321...|\n",
      "|   guess_passwd.|     53|1.081979107187587...|\n",
      "|buffer_overflow.|     30|6.124410040684456E-6|\n",
      "|           land.|     21|4.287087028479119E-6|\n",
      "|    warezmaster.|     20|4.082940027122970...|\n",
      "|           imap.|     12|2.449764016273782...|\n",
      "|        rootkit.|     10|2.041470013561485...|\n",
      "|     loadmodule.|      9|1.837323012205336...|\n",
      "|      ftp_write.|      8|1.633176010849188...|\n",
      "|       multihop.|      7|1.429029009493039...|\n",
      "|            phf.|      4|8.165880054245942E-7|\n",
      "|           perl.|      3|6.124410040684456E-7|\n",
      "|            spy.|      2|4.082940027122971E-7|\n",
      "+----------------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== Check Label Distribution =====\n",
    "label_df = df.groupBy('label').count().orderBy('count', ascending=False)\n",
    "label_df = label_df.withColumn('percent', F.col('count')/F.sum('count').over(Window.partitionBy()))\n",
    "label_df.show(30)   \n",
    "\n",
    "# [NOTE]\n",
    "#- 24 labels\n",
    "#- most of them being smurf and neptune attacks\n",
    "\n",
    "# [수정]\n",
    "#- ratio to % and round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "architectural-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_pipeline(data, used_cols):\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols = used_cols, outputCol='features')\n",
    "    ppl = Pipeline(stages = [assembler])\n",
    "                \n",
    "    featureDf = ppl.fit(data).transform(data)\n",
    "    return featureDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "prepared-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_pipeline(data, str_cols):\n",
    "\n",
    "    stage_string = [StringIndexer(inputCol= c, outputCol= c+\"_str_encoded\") for c in str_cols]\n",
    "    stage_onehot = [OneHotEncoder(inputCol= c+\"_str_encoded\", outputCol= c+ \"_onehot_vec\") for c in str_cols]\n",
    "\n",
    "    ppl = Pipeline(stages = stage_string + stage_onehot)\n",
    "    encodedDF = ppl.fit(data).transform(data)\n",
    "    return encodedDF\n",
    "\n",
    "# [NOTE]\n",
    "#-  use a StringIndexer to turn the categorical values into category indices which are then converted into a column of binary vectors by the OneHotEncoder for each of the columns as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "coordinate-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kmeans_model(data, k):\n",
    "    #-- Train k-mean model\n",
    "    kmeans = KMeans(featuresCol='features', predictionCol='prediction', k=k).setSeed(1)\n",
    "    model = kmeans.fit(data)\n",
    "\n",
    "    #-- Make Predictions\n",
    "    predDf = model.transform(data)\n",
    "\n",
    "    #-- Evaluate clustering by computing Silhouette score\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    silhouette = evaluator.evaluate(predDf)\n",
    "    print(f\"Silhouette with squared euclidean distance with k={k}: {str(silhouette)}\")\n",
    "\n",
    "    #-- Shows the result\n",
    "    centers = model.clusterCenters()\n",
    "#     print(\"\\nCluster Centers: \")\n",
    "#     for center in centers:\n",
    "#         print(center)\n",
    "    return predDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-pontiac",
   "metadata": {},
   "source": [
    "## Basic K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "effective-saturn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in featureDf: 42\n",
      "+-------+---------------------------------------------------------------------------------+\n",
      "|label  |features                                                                         |\n",
      "+-------+---------------------------------------------------------------------------------+\n",
      "|normal.|(38,[1,2,8,19,20,25],[215.0,45076.0,1.0,1.0,1.0,1.0])                            |\n",
      "|normal.|(38,[1,2,8,19,20,25,28,29,30,32],[162.0,4528.0,1.0,2.0,2.0,1.0,1.0,1.0,1.0,1.0]) |\n",
      "|normal.|(38,[1,2,8,19,20,25,28,29,30,32],[236.0,1228.0,1.0,1.0,1.0,1.0,2.0,2.0,1.0,0.5]) |\n",
      "|normal.|(38,[1,2,8,19,20,25,28,29,30,32],[233.0,2032.0,1.0,2.0,2.0,1.0,3.0,3.0,1.0,0.33])|\n",
      "|normal.|(38,[1,2,8,19,20,25,28,29,30,32],[239.0,486.0,1.0,3.0,3.0,1.0,4.0,4.0,1.0,0.25]) |\n",
      "+-------+---------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== Data preprocessing =====\n",
    "num_cols = df.drop('protocol_type', 'service', 'flag', 'label').columns\n",
    "featureDf = feature_pipeline(df, num_cols)\n",
    "\n",
    "print(f\"Number of columns in featureDf: {len(df.columns)}\")\n",
    "featureDf.select(\"label\", \"features\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unavailable-gothic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance with k=2: 0.999998858708827\n",
      "+----------+----------------+-------+\n",
      "|prediction|           label|  count|\n",
      "+----------+----------------+-------+\n",
      "|         0|          smurf.|2807886|\n",
      "|         0|        neptune.|1072017|\n",
      "|         0|         normal.| 972781|\n",
      "|         0|          satan.|  15892|\n",
      "|         0|        ipsweep.|  12481|\n",
      "|         0|      portsweep.|  10412|\n",
      "|         0|           nmap.|   2316|\n",
      "|         0|           back.|   2203|\n",
      "|         0|    warezclient.|   1020|\n",
      "|         0|       teardrop.|    979|\n",
      "|         0|            pod.|    264|\n",
      "|         0|   guess_passwd.|     53|\n",
      "|         0|buffer_overflow.|     30|\n",
      "|         0|           land.|     21|\n",
      "|         0|    warezmaster.|     20|\n",
      "|         0|           imap.|     12|\n",
      "|         0|        rootkit.|     10|\n",
      "|         0|     loadmodule.|      9|\n",
      "|         0|      ftp_write.|      8|\n",
      "|         0|       multihop.|      7|\n",
      "|         0|            phf.|      4|\n",
      "|         0|           perl.|      3|\n",
      "|         0|            spy.|      2|\n",
      "|         1|      portsweep.|      1|\n",
      "+----------+----------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== KMeans Model =====\n",
    "predDf = Kmeans_model(featureDf, k=2)\n",
    "\n",
    "# -- Check Cluster Distribution\n",
    "cluster_df = predDf.select(\"prediction\", \"label\").groupBy('prediction', 'label').count().orderBy('count', ascending=False)\n",
    "cluster_df.show(30)   \n",
    "\n",
    "# [NOTE]\n",
    "#- only one data point ended up in cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "removed-sponsorship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|features_pca                            |\n",
      "+----------------------------------------+\n",
      "|[-228.931024196869,45075.93138650123]   |\n",
      "|[-163.39939875271935,4527.949711690792] |\n",
      "|[-236.3795087236227,1227.927001218682]  |\n",
      "|[-233.62798934133454,2031.9278878244147]|\n",
      "|[-239.15018651748508,485.9261057609585] |\n",
      "+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== PCA ====\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"features_pca\")\n",
    "pca_model = pca.fit(predDf)\n",
    "\n",
    "pcaDf = pca_model.transform(predDf).select(\"features_pca\")\n",
    "pcaDf.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-avatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pca_df = pcaDf.toPandas()\n",
    "dataX = []\n",
    "dataY = []\n",
    "for vec in pca_df.values:\n",
    "    dataX.extend([vec[0][0]])\n",
    "    dataY.extend([vec[0][1]])\n",
    "sns.scatterplot(dataX, dataY, hue=dataY, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PCA ====\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"features_pca\")\n",
    "pca_model = pca.fit(predDf)\n",
    "\n",
    "pcaDf = pca_model.transform(predDf).select(\"features_pca\")\n",
    "pcaDf.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pcaDf.toPandas()\n",
    "dataX = []\n",
    "dataY = []\n",
    "dataZ = []\n",
    "for vec in pca_df.values:\n",
    "    dataX.extend([vec[0][0]])\n",
    "    dataY.extend([vec[0][1]])\n",
    "    dataZ.extend([vec[0][2]])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111, projection='3d') # Axe3D object\n",
    "ax.scatter(dataX, dataY, dataZ, c=dataZ, s=20, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-switch",
   "metadata": {},
   "source": [
    "## Pyspark K-means Clustering with appropriate K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====  scores =====\n",
    "cost = list()\n",
    "for k in [20, 40, 60, 80, 100]:\n",
    "    kmeans = KMeans(featuresCol='features', predictionCol='prediction', k=k).setSeed(1)\n",
    "    model = kmeans.fit(featureDf)\n",
    "    predDf = model.transform(featureDf)\n",
    "    \n",
    "    evaluator = ClusteringEvaluator()\n",
    "    evaluator.setPredictionCol('prediction')\n",
    "    cost.append(evaluator.evaluate(predDf))\n",
    "    print(k, evaluator.evaluate(predDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(20,101, 20), cost, 'o-')\n",
    "ax.set_xlabel('Number of clusters')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Clustering Scores for k-means clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-subscriber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Silhouette scores =====\n",
    "cost = list()\n",
    "evaluator = ClusteringEvaluator()\n",
    "for k in range(2,101):\n",
    "    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "    model = kmeans.fit(featureDf)\n",
    "    predictions = model.transform(featureDf)\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    cost.append(silhouette)\n",
    "    \n",
    "kIdx = np.argmax(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(2,101), cost, 'o-')\n",
    "ax.plot(range(2,101)[kIdx], cost[kIdx]\n",
    "        , marker='o', markersize=14\n",
    "        , markeredgewidth=2, markeredgecolor='r', markerfacecolor='None')\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_xlabel('Number of clusters')\n",
    "ax.set_ylabel('Silhouette Coefficient')\n",
    "ax.set_title('Silhouette Scores for k-means clustering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== KMeans Model =====\n",
    "predDf = Kmeans_model(featureDf, k=20)\n",
    "\n",
    "# -- Check Cluster Distribution\n",
    "cluster_df = predDf.select(\"prediction\", \"label\").groupBy('prediction', 'label').count().orderBy('count', ascending=False)\n",
    "cluster_df.show(30)   \n",
    "\n",
    "# [NOTE]\n",
    "#- only one data point ended up in cluster 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-valley",
   "metadata": {},
   "source": [
    "### Scaling and Encoding\n",
    "#- scalers are applied on Vector Data Types that is why we need to collect the features using a VectorAssembler first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Data preprocessing =====\n",
    "str_cols = [\"protocol_type\", \"service\", \"flag\", \"label\"]\n",
    "encodedDF = encoding_pipeline(df, str_cols)\n",
    "\n",
    "used_cols = df.drop(*str_cols).columns + [c+ \"_onehot_vec\" for c in str_cols]\n",
    "featureDf_enc = feature_pipeline(encodedDF, used_cols)\n",
    "\n",
    "print(f\"Number of columns used: {len(used_cols)}\")\n",
    "featureDf_enc.select(\"label\", \"features\").show(5, False)\n",
    "\n",
    "# [NOTE]\n",
    "#-  use a StringIndexer to turn the categorical values into category indices which are then converted into a column of binary vectors by the OneHotEncoder for each of the columns as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(featureDf_enc)\n",
    "featureDf_scaled = scaler_model.transform(featureDf_enc)\n",
    "\n",
    "# A StandardScaler standardizes features by removing the mean and scaling to unit standard deviation using column-summary-statistics.\n",
    "# StandardScaler can take two additional parameters:\n",
    "# withStd: True by default. Scales the data to unit standard deviation.\n",
    "# withMean: False by default. Centers the data with mean before scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-product",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== KMeans Model =====\n",
    "predDf = Kmeans_model(featureDf_scaled, k=20)\n",
    "\n",
    "# -- Check Cluster Distribution\n",
    "cluster_df = predDf.select(\"prediction\", \"label\").groupBy('prediction', 'label').count().orderBy('count', ascending=False)\n",
    "cluster_df.show(30)   \n",
    "\n",
    "# [NOTE]\n",
    "#- only one data point ended up in cluster 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-invite",
   "metadata": {},
   "source": [
    "## Dimension Reduction: PCA\n",
    "- procedure that converts a set of observations from m to n dimensions (m > n), after analyzing the correlated features of the variables. It is used to move the data from high to a low dimension for visualization or dimensionality reduction purposes.\n",
    "- our data has 42 features but we will reduce to 2 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PCA ====\n",
    "pca = PCA(k=20, inputCol=\"features\", outputCol=\"features_pca\")\n",
    "pca_model = pca.fit(predDf)\n",
    "\n",
    "pcaDf = pca_model.transform(predDf).select(\"features_pca\")\n",
    "pcaDf.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pcaDf.toPandas()\n",
    "dataX = []\n",
    "dataY = []\n",
    "for vec in pca_df.values:\n",
    "    dataX.extend([vec[0][0]])\n",
    "    dataY.extend([vec[0][1]])\n",
    "sns.scatterplot(dataX, dataY, hue=dataY, )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PCA ====\n",
    "pca = PCA(k=3, inputCol=\"features\", outputCol=\"features_pca\")\n",
    "pca_model = pca.fit(predDf)\n",
    "\n",
    "pcaDf = pca_model.transform(predDf).select(\"features_pca\")\n",
    "pcaDf.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pcaDf.toPandas()\n",
    "dataX = []\n",
    "dataY = []\n",
    "dataZ = []\n",
    "for vec in pca_df.values:\n",
    "    dataX.extend([vec[0][0]])\n",
    "    dataY.extend([vec[0][1]])\n",
    "    dataZ.extend([vec[0][2]])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111, projection='3d') # Axe3D object\n",
    "ax.scatter(dataX, dataY, dataZ, c=dataZ, s=20, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-laugh",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
